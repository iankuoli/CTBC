{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion from `EdgeAttibute` to `EdgeEmbeddings` via `GammaVAE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "ModelConfig = namedtuple('ModelConfig', ['input_dims', 'latent_dims', 'hidden_dims', 'gamma_shape', 'prior_shape', 'prior_rate', 'prior_weight'])\n",
    "model_config = ModelConfig(\n",
    "    input_dims = 8,\n",
    "    latent_dims = 3,\n",
    "    hidden_dims = [16, 8, 5],\n",
    "    gamma_shape = 8.,\n",
    "    prior_shape = 2.0,\n",
    "    prior_rate = 1.,\n",
    "    prior_weight = 0.001,\n",
    ")\n",
    "\n",
    "TrainConfig = namedtuple('TrainConfig', ['training_epochs', 'batch_size', 'learning_rate'])\n",
    "train_config = TrainConfig(\n",
    "    training_epochs = 200,\n",
    "    batch_size = 1000,\n",
    "    learning_rate = 1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of `GammaVAE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Gamma\n",
    "from torch.nn import functional as F\n",
    "import torch.tensor as Tensor\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class GammaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: list = None,\n",
    "                 gamma_shape: float = 8.,\n",
    "                 prior_shape: float = 2.0,\n",
    "                 prior_rate: float = 1.,\n",
    "                 prior_weight: float = 0.1,\n",
    "                 **kwargs) -> None:\n",
    "        super(GammaVAE, self).__init__()\n",
    "        \n",
    "        #\n",
    "        # Parameters setting\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        self.input_dim = in_channels\n",
    "        self.latent_dim = latent_dim\n",
    "        self.B = gamma_shape\n",
    "        self.prior_alpha = torch.tensor([prior_shape])\n",
    "        self.prior_beta = torch.tensor([prior_rate])\n",
    "        self.prior_weight = prior_weight\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "        \n",
    "        #\n",
    "        # Build Encoder\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_channels, out_features=h_dim),\n",
    "                    nn.BatchNorm1d(h_dim),\n",
    "                    nn.ELU(),\n",
    "                ))\n",
    "            in_channels = h_dim\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Sequential(nn.Linear(hidden_dims[-1], latent_dim),\n",
    "                                   nn.Softmax())\n",
    "        self.fc_var = nn.Sequential(nn.Linear(hidden_dims[-1], latent_dim),\n",
    "                                    nn.Softmax())\n",
    "\n",
    "        #\n",
    "        # Build Decoder\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Sequential(nn.Linear(latent_dim, hidden_dims[-1]))\n",
    "        hidden_dims = hidden_dims[::-1]\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_features=hidden_dims[i], out_features=hidden_dims[i + 1]),\n",
    "                    nn.BatchNorm1d(hidden_dims[i + 1]),\n",
    "                    nn.ELU(),\n",
    "                ))\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_dims[-1], out_features=self.input_dim),\n",
    "            nn.BatchNorm1d(self.input_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                init_(m)\n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "\n",
    "        # Split the result into mu and var components of the latent Gaussian distribution\n",
    "        alpha = self.fc_mu(result)\n",
    "        beta = self.fc_var(result)\n",
    "\n",
    "        return [alpha, beta]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, alpha: Tensor, beta: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterize the Gamma distribution by the shape augmentation trick.\n",
    "        Reference:\n",
    "        [1] https://arxiv.org/pdf/1610.05683.pdf\n",
    "\n",
    "        :param alpha: (Tensor) Shape parameter of the latent Gamma\n",
    "        :param beta: (Tensor) Rate parameter of the latent Gamma\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Sample from Gamma to guarantee acceptance\n",
    "        alpha_ = alpha.clone().detach()\n",
    "        z_hat = Gamma(alpha_ + self.B, torch.ones_like(alpha_)).sample()\n",
    "\n",
    "        # Compute the eps ~ N(0,1) that produces z_hat\n",
    "        eps = self.inv_h_func(alpha + self.B , z_hat)\n",
    "        z = self.h_func(alpha + self.B, eps)\n",
    "\n",
    "        # When beta != 1, scale by beta\n",
    "        return z / beta\n",
    "\n",
    "    @staticmethod\n",
    "    def h_func(alpha: Tensor, eps: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterize a sample eps ~ N(0, 1) so that h(z) ~ Gamma(alpha, 1)\n",
    "        :param alpha: (Tensor) Shape parameter\n",
    "        :param eps: (Tensor) Random sample to reparameterize\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "\n",
    "        z = (alpha - 1./3.) * (1 + eps / torch.sqrt(9. * alpha - 3.))**3\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def inv_h_func(alpha: Tensor, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inverse reparameterize the given z into eps.\n",
    "        :param alpha: (Tensor)\n",
    "        :param z: (Tensor)\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        eps = torch.sqrt(9. * alpha - 3.) * ((z / (alpha - 1./3.))**(1. / 3.) - 1.)\n",
    "        return eps\n",
    "\n",
    "    @staticmethod\n",
    "    def I_function(a, b, c, d):\n",
    "        return - c * d / a - b * torch.log(a) - torch.lgamma(b) + (b - 1) * (torch.digamma(d) + torch.log(c))\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
    "        alpha, beta = self.encode(input)\n",
    "        z = self.reparameterize(alpha, beta)\n",
    "        return [self.decode(z), input, alpha, beta]\n",
    "\n",
    "    def vae_gamma_kl_loss(self, a, b, c, d):\n",
    "        \"\"\"\n",
    "        https://stats.stackexchange.com/questions/11646/kullback-leibler-divergence-between-two-gamma-distributions\n",
    "        b and d are Gamma shape parameters and\n",
    "        a and c are scale parameters.\n",
    "        (All, therefore, must be positive.)\n",
    "        \"\"\"\n",
    "\n",
    "        a = 1 / a\n",
    "        c = 1 / c\n",
    "        losses = self.I_function(c, d, c, d) - self.I_function(a, b, c, d)\n",
    "        return torch.sum(losses, dim=1)\n",
    "\n",
    "    def loss_function(self, *args, **kwargs) -> dict:\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        alpha = args[2]\n",
    "        beta = args[3]\n",
    "\n",
    "        curr_device = input.device\n",
    "        recons_loss = torch.mean(F.mse_loss(recons, input, reduction='none'), dim=(1,))\n",
    "\n",
    "        self.prior_alpha = self.prior_alpha.to(curr_device)\n",
    "        self.prior_beta = self.prior_beta.to(curr_device)\n",
    "\n",
    "        kld_loss = self.vae_gamma_kl_loss(alpha, beta, self.prior_alpha, self.prior_beta)\n",
    "\n",
    "        loss = (1 - self.prior_weight) * recons_loss + self.prior_weight * kld_loss\n",
    "        loss = torch.mean(loss, dim = 0)\n",
    "        # print(loss, recons_loss, kld_loss)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def sample(self, num_samples:int, current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the modelSay\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = Gamma(self.prior_alpha, self.prior_beta).sample((num_samples, self.latent_dim))\n",
    "        z = z.squeeze().to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "\n",
    "def init_(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        init.orthogonal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = glob.glob('Edge_Attribute/*.csv')\n",
    "dfs = [pd.read_csv(url, encoding = \"ISO-8859-1\", engine='python') for url in urls]\n",
    "df_train = pd.concat(dfs[:16])\n",
    "df_test  = pd.concat(dfs[16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Large_TransOut_Count</th>\n",
       "      <th>TransOut_Count</th>\n",
       "      <th>Total_Large_TransIn</th>\n",
       "      <th>Total_WireTrans</th>\n",
       "      <th>Total_WireTrans_Times</th>\n",
       "      <th>Average_WireTrans</th>\n",
       "      <th>WireTransIn_8000</th>\n",
       "      <th>WireTrans_Out_9mon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1261999.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>515242.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>515242.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675792.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2347140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2347140.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10128640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1258224.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Large_TransOut_Count  TransOut_Count  Total_Large_TransIn  Total_WireTrans  \\\n",
       "0                   0.0             0.0           1261999.04              0.0   \n",
       "1                   1.0             1.0                 0.00         515242.0   \n",
       "2                   0.0             1.0                 0.00            150.0   \n",
       "3                   1.0             1.0                 0.00        2347140.0   \n",
       "4                   0.0             0.0                 0.00              0.0   \n",
       "\n",
       "   Total_WireTrans_Times  Average_WireTrans  WireTransIn_8000  \\\n",
       "0                    0.0                0.0               0.0   \n",
       "1                    1.0           515242.0               0.0   \n",
       "2                    1.0              150.0               0.0   \n",
       "3                    1.0          2347140.0               0.0   \n",
       "4                    0.0                0.0               0.0   \n",
       "\n",
       "   WireTrans_Out_9mon  \n",
       "0                 0.0  \n",
       "1            675792.0  \n",
       "2                 0.0  \n",
       "3          10128640.0  \n",
       "4           1258224.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr_train = df_train.iloc[:,2:]\n",
    "edge_attr_test  = df_test.iloc[:,2:]\n",
    "edge_attr_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Large_TransOut_Count</th>\n",
       "      <th>TransOut_Count</th>\n",
       "      <th>Total_Large_TransIn</th>\n",
       "      <th>Total_WireTrans</th>\n",
       "      <th>Total_WireTrans_Times</th>\n",
       "      <th>Average_WireTrans</th>\n",
       "      <th>WireTransIn_8000</th>\n",
       "      <th>WireTrans_Out_9mon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.048208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.152394</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>13.152394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.423642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.017280</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>5.017280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.668709</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>14.668709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.130878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.045213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Large_TransOut_Count  TransOut_Count  Total_Large_TransIn  Total_WireTrans  \\\n",
       "0              0.000000        0.000000            14.048208         0.000000   \n",
       "1              0.693147        0.693147             0.000000        13.152394   \n",
       "2              0.000000        0.693147             0.000000         5.017280   \n",
       "3              0.693147        0.693147             0.000000        14.668709   \n",
       "4              0.000000        0.000000             0.000000         0.000000   \n",
       "\n",
       "   Total_WireTrans_Times  Average_WireTrans  WireTransIn_8000  \\\n",
       "0               0.000000           0.000000               0.0   \n",
       "1               0.693147          13.152394               0.0   \n",
       "2               0.693147           5.017280               0.0   \n",
       "3               0.693147          14.668709               0.0   \n",
       "4               0.000000           0.000000               0.0   \n",
       "\n",
       "   WireTrans_Out_9mon  \n",
       "0            0.000000  \n",
       "1           13.423642  \n",
       "2            0.000000  \n",
       "3           16.130878  \n",
       "4           14.045213  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr_train = np.log(edge_attr_train + 1.)\n",
    "edge_attr_test = np.log(edge_attr_test + 1.)\n",
    "edge_attr_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "edge_attr_train_scaled = scaler.fit_transform(edge_attr_train)\n",
    "edge_attr_test_scaled  = scaler.transform(edge_attr_test)\n",
    "# AML_dataset = pd.DataFrame(features, columns=AML_dataset.columns, index=AML_dataset.index)\n",
    "# AML_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_train_scaled = pd.DataFrame(edge_attr_train_scaled)\n",
    "edge_attr_train_scaled = torch.tensor(edge_attr_train_scaled.values, dtype=torch.float32).cuda()\n",
    "\n",
    "edge_attr_test_scaled = pd.DataFrame(edge_attr_test_scaled)\n",
    "edge_attr_test_scaled = torch.tensor(edge_attr_test_scaled.values, dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Optimizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = GammaVAE(in_channels=model_config.input_dims,\n",
    "               latent_dim=model_config.latent_dims,\n",
    "               hidden_dims=model_config.hidden_dims,\n",
    "               gamma_shape=model_config.gamma_shape,\n",
    "               prior_shape=model_config.prior_shape,\n",
    "               prior_rate=model_config.prior_rate,\n",
    "               prior_weight=model_config.prior_weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=train_config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 Training loss= 0.732802212 Testing loss= 0.711290300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "indice = torch.tensor(random.sample(range(edge_attr_train_scaled.shape[0]), train_config.batch_size))\n",
    "batch_xs = edge_attr_train_scaled[indice]\n",
    "x_output, x_input, x_alpha, x_beta = vae(batch_xs)\n",
    "loss_dict = vae.loss_function(x_output, x_input, x_alpha, x_beta)\n",
    "train_loss_ini = loss_dict['loss']\n",
    "\n",
    "indice = torch.tensor(random.sample(range(edge_attr_test_scaled.shape[0]), train_config.batch_size))\n",
    "batch_xs = edge_attr_test_scaled[indice]\n",
    "x_output, x_input, x_alpha, x_beta = vae(batch_xs)\n",
    "loss_dict = vae.loss_function(x_output, x_input, x_alpha, x_beta)\n",
    "test_loss_ini = loss_dict['loss']\n",
    "\n",
    "print(\"Epoch:\", '%04d' % (0), \n",
    "      \"Training loss=\", \"{:.9f}\".format(train_loss_ini), \n",
    "      \"Testing loss=\", \"{:.9f}\".format(test_loss_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Training loss= 0.630568266 Testing loss= 0.000511591\n",
      "Epoch: 0002 Training loss= 0.514774740 Testing loss= 0.000431419\n",
      "Epoch: 0003 Training loss= 0.441184103 Testing loss= 0.000372224\n",
      "Epoch: 0004 Training loss= 0.387676656 Testing loss= 0.000346573\n",
      "Epoch: 0005 Training loss= 0.346536309 Testing loss= 0.000291239\n",
      "Epoch: 0006 Training loss= 0.304940552 Testing loss= 0.000262089\n",
      "Epoch: 0007 Training loss= 0.265059531 Testing loss= 0.000213156\n",
      "Epoch: 0008 Training loss= 0.217041194 Testing loss= 0.000183434\n",
      "Epoch: 0009 Training loss= 0.194039866 Testing loss= 0.000163344\n",
      "Epoch: 0010 Training loss= 0.176967680 Testing loss= 0.000150220\n",
      "Epoch: 0011 Training loss= 0.167441070 Testing loss= 0.000140745\n",
      "Epoch: 0012 Training loss= 0.160921603 Testing loss= 0.000143355\n",
      "Epoch: 0013 Training loss= 0.155339211 Testing loss= 0.000137041\n",
      "Epoch: 0014 Training loss= 0.149634436 Testing loss= 0.000128531\n",
      "Epoch: 0015 Training loss= 0.143482685 Testing loss= 0.000122143\n",
      "Epoch: 0016 Training loss= 0.136198372 Testing loss= 0.000116215\n",
      "Epoch: 0017 Training loss= 0.129140526 Testing loss= 0.000111485\n",
      "Epoch: 0018 Training loss= 0.123854250 Testing loss= 0.000111587\n",
      "Epoch: 0019 Training loss= 0.119412810 Testing loss= 0.000105724\n",
      "Epoch: 0020 Training loss= 0.115495026 Testing loss= 0.000103411\n",
      "Epoch: 0021 Training loss= 0.112046398 Testing loss= 0.000095386\n",
      "Epoch: 0022 Training loss= 0.108872198 Testing loss= 0.000095130\n",
      "Epoch: 0023 Training loss= 0.105421253 Testing loss= 0.000090862\n",
      "Epoch: 0024 Training loss= 0.102062136 Testing loss= 0.000085554\n",
      "Epoch: 0025 Training loss= 0.097431384 Testing loss= 0.000082757\n",
      "Epoch: 0026 Training loss= 0.091149002 Testing loss= 0.000080513\n",
      "Epoch: 0027 Training loss= 0.065272436 Testing loss= 0.000041961\n",
      "Epoch: 0028 Training loss= 0.047929272 Testing loss= 0.000038651\n",
      "Epoch: 0029 Training loss= 0.044627175 Testing loss= 0.000035322\n",
      "Epoch: 0030 Training loss= 0.042830843 Testing loss= 0.000032349\n",
      "Epoch: 0031 Training loss= 0.039682209 Testing loss= 0.000032361\n",
      "Epoch: 0032 Training loss= 0.038311966 Testing loss= 0.000030112\n",
      "Epoch: 0033 Training loss= 0.035021234 Testing loss= 0.000027137\n",
      "Epoch: 0034 Training loss= 0.032873321 Testing loss= 0.000121515\n",
      "Epoch: 0035 Training loss= 0.030367464 Testing loss= 0.000023756\n",
      "Epoch: 0036 Training loss= 0.029002208 Testing loss= 0.000021878\n",
      "Epoch: 0037 Training loss= 0.026609147 Testing loss= 0.000019962\n",
      "Epoch: 0038 Training loss= 0.026294099 Testing loss= 0.000020914\n",
      "Epoch: 0039 Training loss= 0.024458945 Testing loss= 0.000017045\n",
      "Epoch: 0040 Training loss= 0.026036071 Testing loss= 0.000124573\n",
      "Epoch: 0041 Training loss= 0.023673389 Testing loss= 0.000015400\n",
      "Epoch: 0042 Training loss= 0.024119264 Testing loss= 0.000017680\n",
      "Epoch: 0043 Training loss= 0.021700572 Testing loss= 0.000017842\n",
      "Epoch: 0044 Training loss= 0.020602256 Testing loss= 0.000018179\n",
      "Epoch: 0045 Training loss= 0.020684458 Testing loss= 0.000018444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-132a58df0da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m## valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(train_config.training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(edge_attr_train_scaled.shape[0] / train_config.batch_size)\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    avg_train_cost = 0.\n",
    "    avg_test_cost = 0.\n",
    "    for i in range(total_batch):\n",
    "        \n",
    "        ## train\n",
    "        indice = torch.tensor(random.sample(range(edge_attr_train_scaled.shape[0]), train_config.batch_size))\n",
    "        batch_xs = edge_attr_train_scaled[indice]\n",
    "\n",
    "        x_output, x_input, x_alpha, x_beta = vae(batch_xs)\n",
    "        loss_dict = vae.loss_function(x_output, x_input, x_alpha, x_beta)\n",
    "        train_loss.append(loss_dict['loss'] / edge_attr_train_scaled.shape[0] * train_config.batch_size)\n",
    "        avg_train_cost += train_loss[-1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss[-1].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## valid\n",
    "        indice = torch.tensor(random.sample(range(edge_attr_test_scaled.shape[0]), train_config.batch_size))\n",
    "        batch_xs = edge_attr_test_scaled[indice]\n",
    "\n",
    "        x_output, x_input, x_alpha, x_beta = vae(batch_xs)\n",
    "        loss_dict = vae.loss_function(x_output, x_input, x_alpha, x_beta)\n",
    "        test_loss.append(loss_dict['loss'] / edge_attr_train_scaled.shape[0] * train_config.batch_size)\n",
    "        avg_test_cost += test_loss[-1]\n",
    "     \n",
    "    # Display logs per epoch step    \n",
    "    print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "          \"Training loss=\", \"{:.9f}\".format(avg_train_cost), \"Testing loss=\", \"{:.9f}\".format(test_loss[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating `EdgeEmbeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8616, 1.3121, 4.1235],\n",
       "        [1.3659, 1.4379, 0.9330],\n",
       "        [1.8430, 0.8570, 0.9022],\n",
       "        [1.2423, 1.5387, 0.9604],\n",
       "        [0.9004, 0.9434, 1.3315],\n",
       "        [0.9027, 0.9403, 1.3310],\n",
       "        [2.5470, 1.1560, 0.7780],\n",
       "        [2.7016, 1.1012, 0.6829],\n",
       "        [0.8825, 0.9573, 1.3329],\n",
       "        [1.9542, 0.8930, 0.8496]], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_output, train_x_input, train_x_alpha, train_x_beta = vae(edge_attr_train_scaled)\n",
    "train_mean = train_x_alpha / train_x_beta\n",
    "train_mean[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9653, 1.0041, 3.7995],\n",
       "        [0.9655, 1.0037, 3.7990],\n",
       "        [0.9424, 0.8919, 1.2775],\n",
       "        [0.8230, 1.0190, 1.3399],\n",
       "        [1.3687, 1.4174, 0.9472],\n",
       "        [2.7459, 1.1319, 0.7801],\n",
       "        [2.5276, 1.1070, 0.7316],\n",
       "        [2.8289, 1.2349, 0.6895],\n",
       "        [1.4041, 1.4209, 0.9409],\n",
       "        [0.9572, 1.0245, 3.8274]], device='cuda:0', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_output, test_x_input, test_x_alpha, test_x_beta = vae(edge_attr_test_scaled)\n",
    "test_mean = test_x_alpha / test_x_beta\n",
    "test_mean[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9653172 , 1.0041116 , 3.7995434 ],\n",
       "       [0.965473  , 1.0037236 , 3.7989998 ],\n",
       "       [0.9424194 , 0.8918789 , 1.2775328 ],\n",
       "       ...,\n",
       "       [0.9577436 , 1.0230571 , 3.825455  ],\n",
       "       [0.959069  , 1.0197277 , 3.8209865 ],\n",
       "       [0.9877101 , 0.94934356, 3.7180855 ]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean.cpu().data.numpy()\n",
    "test_mean.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1080709, 3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train\n",
    "# df_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080709, 3)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean.cpu().data.numpy().shape\n",
    "# test_mean.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_res = np.concatenate((train_mean.cpu().data.numpy(), test_mean.cpu().data.numpy()), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520631, 3)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAE_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-8 54014\n",
      "2017-9 58190\n",
      "2017-10 62001\n",
      "2017-11 63763\n",
      "2017-12 65844\n",
      "2018-1 68002\n",
      "2018-2 67346\n",
      "2018-3 68397\n",
      "2018-4 70560\n",
      "2018-5 72217\n",
      "2018-6 72674\n",
      "2018-7 72923\n",
      "2018-8 73160\n",
      "2018-9 71512\n",
      "2018-10 70925\n",
      "2018-11 69181\n",
      "2018-12 68906\n",
      "2019-1 68598\n",
      "2019-2 64474\n",
      "2019-3 61642\n",
      "2019-4 60343\n",
      "2019-5 58842\n",
      "2019-6 57117\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "index = 0\n",
    "for url in urls:\n",
    "    date = url.split('_')[-1].split('.')[0]\n",
    "    print (date, dfs[index].shape[0])\n",
    "    tmp_df =  pd.concat([pd.DataFrame(dfs[index].iloc[:,:2].values), \n",
    "                         pd.DataFrame(VAE_res[count:count+dfs[index].shape[0],:])], axis=1)\n",
    "    tmp_df.to_csv('Edge_Embedd_{}.csv'.format(date), header=False)\n",
    "    count += dfs[index].shape[0]\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
