{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Classifier\n",
    "# ---------------------\n",
    "## focal loss\n",
    "alpha = 1\n",
    "gamma_pos = 6\n",
    "gamma_neg = 2\n",
    "grad_clip = 1\n",
    "lambda_l1 = 0\n",
    "weight_decay = 0   # lambda_l2\n",
    "\n",
    "#\n",
    "# VAT\n",
    "# ---------------------\n",
    "vat_xi = 1e-6\n",
    "vat_eps_pos = 1e2\n",
    "vat_eps_neg = 1e-1\n",
    "vat_ip = 1\n",
    "\n",
    "#\n",
    "# Training process\n",
    "# ---------------------\n",
    "train_batch_size = 128\n",
    "test_batch_size = 32\n",
    "\n",
    "#\n",
    "# Optimizer\n",
    "# ---------------------\n",
    "optim_type = 'rlars'       # ['adam', 'rlars']\n",
    "learn_rate = 1e-4\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "max_epochs = 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, fc_dims, in_dim=256, out_dim=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.conv1_dim = math.ceil(self.in_dim)\n",
    "        self.conv2_dim = math.ceil(self.in_dim * 2)\n",
    "        self.conv3_dim = math.ceil(self.in_dim)\n",
    "        self.outdim_en1 = fc_dims[0]\n",
    "        self.outdim_en2 = fc_dims[1]\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.model_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_dim, out_channels=self.conv1_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv1_dim, out_channels=self.conv2_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv2_dim, out_channels=self.conv3_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv3_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dropout_1 = 1 - 0.1\n",
    "        dropout_2 = 1 - 0.2\n",
    "        dropout_3 = 1 - 0.2\n",
    "        self.model_fc = nn.Sequential(\n",
    "            # FC 1\n",
    "            nn.Dropout(dropout_1),\n",
    "            nn.Linear(in_features=self.conv3_dim, out_features=self.outdim_en1),\n",
    "            nn.BatchNorm1d(self.outdim_en1),\n",
    "            nn.ReLU(),\n",
    "            # FC 2\n",
    "            nn.Dropout(dropout_2),\n",
    "            nn.Linear(in_features=self.outdim_en1, out_features=self.outdim_en2),\n",
    "            nn.BatchNorm1d(self.outdim_en2),\n",
    "            nn.ReLU(),\n",
    "            # FC 3\n",
    "            nn.Dropout(dropout_3),\n",
    "            nn.Linear(in_features=self.outdim_en2, out_features=self.out_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "        print(\"conv_dim = [{}, {}, {}]\".format(self.conv1_dim, self.conv2_dim, self.conv3_dim))\n",
    "        print(\"fc_dim = [{}/{}, {}/{}, {}/{}, {}]\".format(self.conv3_dim, dropout_1, \n",
    "                                                          self.outdim_en1, dropout_2, \n",
    "                                                          self.outdim_en2, dropout_3, self.out_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model_conv(x)\n",
    "        x = self.model_fc(x.view(x.shape[0], -1))\n",
    "        return x\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        return (param for param in self.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss2(nn.Module):\n",
    "    def __init__(self, alpha=0.01, gamma_pos=3, gamma_neg=2, logits=False, reduce=True):\n",
    "        super(FocalLoss2, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        gamma_diff = self.gamma_pos - self.gamma_neg\n",
    "        F_loss_pos = self.alpha * targets * (1-pt)**self.gamma_pos * BCE_loss\n",
    "        F_loss_pos = torch.mean(pt)**(-gamma_diff) * F_loss_pos\n",
    "        F_loss_neg = self.alpha * (1 - targets) * (1-pt)**self.gamma_neg * BCE_loss\n",
    "        F_loss = 1 * F_loss_pos + 0.9 * F_loss_neg\n",
    "        \n",
    "        avg_F_loss_pos = torch.sum(F_loss_pos) / torch.sum(targets)\n",
    "        avg_F_loss_neg = torch.sum(F_loss_neg) / torch.sum(1-targets)\n",
    "        \n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss), avg_F_loss_pos, avg_F_loss_neg\n",
    "        else:\n",
    "            return F_loss, F_loss_pos, F_loss_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Testing partition setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_date  = [(2018, 7), (2018, 8), (2018, 9),(2018, 10), (2018, 11), (2018, 12)]\n",
    "testing_date   = [(2019, 1), (2019, 2), (2019, 3), (2019, 4), (2019, 5), (2019, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(map(lambda ym: (ym, \n",
    "                            np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(*ym), allow_pickle=True)), \n",
    "                training_date + testing_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(map(lambda ym: data[ym], training_date))\n",
    "test_data = list(map(lambda ym: data[ym], testing_date))\n",
    "\n",
    "X_train_ = np.concatenate([data['arr_0'] for data in train_data])\n",
    "y_train_ = np.concatenate([data['arr_1'] for data in train_data])\n",
    "training_announce = np.concatenate([data['arr_2'] for data in train_data])\n",
    "training_FILTER = np.concatenate([data['arr_3'] for data in train_data])\n",
    "\n",
    "X_test_ = np.concatenate([data['arr_0'] for data in test_data])\n",
    "y_test_ = np.concatenate([data['arr_1'] for data in test_data])\n",
    "testing_announce = np.concatenate([data['arr_2'] for data in test_data])\n",
    "testing_FILTER = np.concatenate([data['arr_3'] for data in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting announced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_[training_announce == 1]\n",
    "y_train = y_train_[training_announce == 1]\n",
    "\n",
    "X_test = X_test_[testing_announce == 1]\n",
    "y_test = y_test_[testing_announce == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magical rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,:,0] = 0.1 * np.log10(1e4*X_train[:,:,0]**3 + 1e-10) + 1\n",
    "X_train[:,:,2] = 0.1 * np.log10(1e4*X_train[:,:,2]**5 + 1e-10) + 1\n",
    "X_train[:,:,3] = 0.1 * np.log10(1e4*X_train[:,:,3]**2 + 1e-10) + 1\n",
    "X_train[:,:,6] = 0.1 * np.log10(1e0*X_train[:,:,6]**3 + 1e-10) + 1\n",
    "X_train[:,:,7] = 0.1 * np.log10(1e8*X_train[:,:,7]**5 + 1e-10) + 1\n",
    "X_train[:,:,8] = 0.1 * np.log10(1e8*X_train[:,:,8]**3 + 1e-10) + 1\n",
    "X_train[:,:,9] = 0.1 * np.log10(1e8*X_train[:,:,9]**6 + 1e-10) + 1\n",
    "X_train[:,:,10] = 0.1 * np.log10(1e8*X_train[:,:,10]**2 + 1e-10) + 1\n",
    "X_train[:,:,12] = 0.1 * np.log10(1e9*X_train[:,:,12]**3 + 1e-10) + 1\n",
    "X_train[:,:,13] = 0.1 * np.log10(1e9*X_train[:,:,13]**5 + 1e-10) + 1\n",
    "X_train[:,:,14] = 0.1 * np.log10(1e9*X_train[:,:,14]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,15] = 0.1 * np.log10(1e9*X_train[:,:,15]**6 + 1e-10) + 1\n",
    "X_train[:,:,16] = 0.1 * np.log10(1e15*X_train[:,:,16]**3 + 1e-10) + 1\n",
    "X_train[:,:,18] = 0.1 * np.log10(1e15*X_train[:,:,18]**4 + 1e-10) + 1\n",
    "X_train[:,:,20] = 0.1 * np.log10(1e10*X_train[:,:,20]**8 + 1e-10) + 1\n",
    "X_train[:,:,21] = 0.1 * np.log10(1e8*X_train[:,:,21]**3 + 1e-10) + 1\n",
    "X_train[:,:,22] = 0.1 * np.log10(1e20*X_train[:,:,22]**4 + 1e-10) + 1\n",
    "X_train[:,:,23] = 0.1 * np.log10(1e20*X_train[:,:,23]**4 + 1e-10) + 1\n",
    "X_train[:,:,24] = 0.1 * np.log10(1e10*X_train[:,:,24]**5 + 1e-10) + 1\n",
    "X_train[:,:,26] = 0.1 * np.log10(1e10*X_train[:,:,26]**7 + 1e-10) + 1\n",
    "X_train[:,:,27] = 0.1 * np.log10(1e10*X_train[:,:,27]**3 + 1e-10) + 1\n",
    "X_train[:,:,29] = 0.1 * np.log10(1e10*X_train[:,:,29]**7 + 1e-10) + 1\n",
    "X_train[:,:,30] = 0.1 * np.log10(1e10*X_train[:,:,30]**3 + 1e-10) + 1\n",
    "X_train[:,:,32] = 0.1 * np.log10(1e10*X_train[:,:,32]**3 + 1e-10) + 1\n",
    "X_train[:,:,33] = 0.1 * np.log10(1e10*X_train[:,:,33]**2.5 + 1e-10) + 1\n",
    "X_train[:,:,34] = 0.1 * np.log10(1e10*X_train[:,:,34]**3 + 1e-10) + 1\n",
    "X_train[:,:,35] = 0.1 * np.log10(1e20*X_train[:,:,35]**4 + 1e-10) + 1\n",
    "X_train[:,:,36] = 0.1 * np.log10(1e18*X_train[:,:,36]**5 + 1e-10) + 1\n",
    "X_train[:,:,37] = 0.1 * np.log10(1e20*X_train[:,:,37]**4 + 1e-10) + 1\n",
    "X_train[:,:,38] = 0.1 * np.log10(1e20*X_train[:,:,38]**3 + 1e-10) + 1\n",
    "X_train[:,:,39] = 0.1 * np.log10(1e20*X_train[:,:,39]**3 + 1e-10) + 1\n",
    "X_train[:,:,40] = 0.1 * np.log10(1e20*X_train[:,:,40]**3 + 1e-10) + 1\n",
    "X_train[:,:,41] = 0.1 * np.log10(1e20*X_train[:,:,41]**3 + 1e-10) + 1\n",
    "X_train[:,:,42] = 0.1 * np.log10(1e20*X_train[:,:,42]**3 + 1e-10) + 1\n",
    "X_train[:,:,43] = 0.1 * np.log10(1e20*X_train[:,:,43]**3 + 1e-10) + 1\n",
    "X_train[:,:,44] = 0.1 * np.log10(1e20*X_train[:,:,44]**3 + 1e-10) + 1\n",
    "X_train[:,:,45] = 0.1 * np.log10(1e20*X_train[:,:,45]**3 + 1e-10) + 1\n",
    "X_train[:,:,46] = 0.1 * np.log10(1e20*X_train[:,:,46]**3 + 1e-10) + 1\n",
    "X_train[:,:,47] = 0.1 * np.log10(1e20*X_train[:,:,47]**3 + 1e-10) + 1\n",
    "X_train[:,:,51] = 0.1 * np.log10(1e20*X_train[:,:,51]**3 + 1e-10) + 1\n",
    "X_train[:,:,52] = 0.1 * np.log10(1e20*X_train[:,:,52]**3 + 1e-10) + 1\n",
    "X_train[:,:,53] = 0.1 * np.log10(1e20*X_train[:,:,53]**3 + 1e-10) + 1\n",
    "X_train[:,:,54] = 0.1 * np.log10(1e20*X_train[:,:,54]**3 + 1e-10) + 1\n",
    "X_train[:,:,57] = 0.1 * np.log10(1e20*X_train[:,:,57]**20 + 1e-10) + 1\n",
    "X_train[:,:,58] = 0.1 * np.log10(1e20*X_train[:,:,58]**10 + 1e-10) + 1\n",
    "X_train[:,:,59] = 0.1 * np.log10(1e20*X_train[:,:,59]**8 + 1e-10) + 1\n",
    "X_train[:,:,60] = 0.1 * np.log10(1e20*X_train[:,:,60]**6 + 1e-10) + 1\n",
    "X_train[:,:,61] = 0.1 * np.log10(1e20*X_train[:,:,61]**6 + 1e-10) + 1\n",
    "X_train[:,:,62] = 0.1 * np.log10(1e20*X_train[:,:,62]**5 + 1e-10) + 1\n",
    "X_train[:,:,63] = 0.1 * np.log10(1e20*X_train[:,:,63]**3 + 1e-10) + 1\n",
    "X_train[:,:,64] = 0.1 * np.log10(1e20*X_train[:,:,64]**3 + 1e-10) + 1\n",
    "X_train[:,:,65] = 0.1 * np.log10(1e20*X_train[:,:,65]**3 + 1e-10) + 1\n",
    "X_train[:,:,66] = 0.1 * np.log10(1e20*X_train[:,:,66]**3 + 1e-10) + 1\n",
    "X_train[:,:,67] = 0.1 * np.log10(1e20*X_train[:,:,67]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,68] = 0.1 * np.log10(1e20*X_train[:,:,68]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,69] = 0.1 * np.log10(1e20*X_train[:,:,69]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,70] = 0.1 * np.log10(1e20*X_train[:,:,70]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,71] = 0.1 * np.log10(1e20*X_train[:,:,71]**3 + 1e-10) + 1\n",
    "X_train[:,:,72] = 0.1 * np.log10(1e20*X_train[:,:,72]**3 + 1e-10) + 1\n",
    "X_train[:,:,73] = 0.1 * np.log10(1e20*X_train[:,:,73]**3 + 1e-10) + 1\n",
    "X_train[:,:,74] = 0.1 * np.log10(1e20*X_train[:,:,74]**3 + 1e-10) + 1\n",
    "X_train[:,:,75] = 0.1 * np.log10(1e20*X_train[:,:,75]**3 + 1e-10) + 1\n",
    "X_train[:,:,76] = 0.1 * np.log10(1e20*X_train[:,:,76]**3 + 1e-10) + 1\n",
    "X_train[:,:,77] = 0.1 * np.log10(1e15*X_train[:,:,77]**8 + 1e-10) + 1\n",
    "X_train[:,:,78] = 0.1 * np.log10(1e15*X_train[:,:,78]**8 + 1e-10) + 1\n",
    "X_train[:,:,79] = 0.1 * np.log10(1e20*X_train[:,:,79]**5 + 1e-10) + 1\n",
    "X_train[:,:,80] = 0.1 * np.log10(1e20*X_train[:,:,80]**6 + 1e-10) + 1\n",
    "X_train[:,:,81] = 0.1 * np.log10(1e20*X_train[:,:,81]**8 + 1e-10) + 1\n",
    "X_train[:,:,82] = 0.1 * np.log10(1e20*X_train[:,:,82]**10 + 1e-10) + 1\n",
    "X_train[:,:,83] = 0.1 * np.log10(1e20*X_train[:,:,83]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,84] = 0.1 * np.log10(1e20*X_train[:,:,84]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,85] = 0.1 * np.log10(1e20*X_train[:,:,85]**3 + 1e-10) + 1\n",
    "X_train[:,:,86] = 0.1 * np.log10(1e20*X_train[:,:,86]**13 + 1e-10) + 1\n",
    "X_train[:,:,87] = 0.1 * np.log10(1e20*X_train[:,:,87]**10 + 1e-10) + 1\n",
    "X_train[:,:,88] = 0.1 * np.log10(1e20*X_train[:,:,88]**9 + 1e-10) + 1\n",
    "X_train[:,:,89] = 0.1 * np.log10(1e20*X_train[:,:,89]**8 + 1e-10) + 1\n",
    "X_train[:,:,91] = X_train[:,:,91] ** 5\n",
    "X_train[:,:,93] = X_train[:,:,93] ** 2\n",
    "X_train[:,:,94] = X_train[:,:,94] ** 2\n",
    "X_train[:,:,96] = 0.1 * np.log10(1e20*X_train[:,:,96]**4 + 1e-10) + 1\n",
    "X_train[:,:,97] = 0.1 * np.log10(1e20*X_train[:,:,97]**9 + 1e-10) + 1\n",
    "X_train[:,:,98] = 0.1 * np.log10(1e20*X_train[:,:,98]**8 + 1e-10) + 1\n",
    "X_train[:,:,99] = 0.1 * np.log10(1e20*X_train[:,:,99]**7 + 1e-10) + 1\n",
    "X_train[:,:,100] = 0.1 * np.log10(1e20*X_train[:,:,100]**7 + 1e-10) + 1\n",
    "X_train[:,:,101] = 0.1 * np.log10(1e20*X_train[:,:,101]**5 + 1e-10) + 1\n",
    "X_train[:,:,102] = 0.1 * np.log10(1e20*X_train[:,:,102]**4 + 1e-10) + 1\n",
    "X_train[:,:,103] = 0.1 * np.log10(1e20*X_train[:,:,103]**4 + 1e-10) + 1\n",
    "X_train[:,:,104] = 0.1 * np.log10(1e20*X_train[:,:,104]**10 + 1e-10) + 1\n",
    "X_train[:,:,106] = 0.1 * np.log10(1e20*X_train[:,:,106]**4 + 1e-10) + 1\n",
    "X_train[:,:,107] = 0.1 * np.log10(1e20*X_train[:,:,107]**8 + 1e-10) + 1\n",
    "X_train[:,:,108] = 0.1 * np.log10(1e20*X_train[:,:,108]**7 + 1e-10) + 1\n",
    "X_train[:,:,109] = 0.1 * np.log10(1e20*X_train[:,:,109]**7 + 1e-10) + 1\n",
    "X_train[:,:,110] = 0.1 * np.log10(1e20*X_train[:,:,110]**7 + 1e-10) + 1\n",
    "X_train[:,:,111] = 0.1 * np.log10(1e20*X_train[:,:,111]**6 + 1e-10) + 1\n",
    "X_train[:,:,112] = 0.1 * np.log10(1e20*X_train[:,:,112]**5 + 1e-10) + 1\n",
    "X_train[:,:,113] = 0.1 * np.log10(1e20*X_train[:,:,113]**5 + 1e-10) + 1\n",
    "X_train[:,:,114] = 0.1 * np.log10(1e20*X_train[:,:,114]**8 + 1e-10) + 1\n",
    "X_train[:,:,115] = 0.1 * np.log10(1e20*X_train[:,:,115]**7 + 1e-10) + 1\n",
    "X_train[:,:,116] = 0.1 * np.log10(1e20*X_train[:,:,116]**6 + 1e-10) + 1\n",
    "X_train[:,:,117] = 0.1 * np.log10(1e20*X_train[:,:,117]**6 + 1e-10) + 1\n",
    "X_train[:,:,118] = 0.1 * np.log10(1e20*X_train[:,:,118]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,119] = 0.1 * np.log10(1e20*X_train[:,:,119]**4 + 1e-10) + 1\n",
    "X_train[:,:,124] = 0.1 * np.log10(1e20*X_train[:,:,124]**6 + 1e-10) + 1\n",
    "X_train[:,:,125] = 0.1 * np.log10(1e20*X_train[:,:,125]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,126] = 0.1 * np.log10(1e20*X_train[:,:,126]**11 + 1e-10) + 1\n",
    "X_train[:,:,127] = 0.1 * np.log10(1e20*X_train[:,:,127]**8 + 1e-10) + 1\n",
    "X_train[:,:,128] = 0.1 * np.log10(1e20*X_train[:,:,128]**8 + 1e-10) + 1\n",
    "X_train[:,:,129] = 0.1 * np.log10(1e20*X_train[:,:,129]**8 + 1e-10) + 1\n",
    "X_train[:,:,130] = 0.1 * np.log10(1e20*X_train[:,:,130]**8 + 1e-10) + 1\n",
    "X_train[:,:,132] = 0.1 * np.log10(1e12*X_train[:,:,132]**12 + 1e-10) + 1\n",
    "X_train[:,:,133] = 0.1 * np.log10(1e20*X_train[:,:,133]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,140] = 0.1 * np.log10(1e20*X_train[:,:,140]**4 + 1e-10) + 1\n",
    "X_train[:,:,141] = 0.1 * np.log10(1e20*X_train[:,:,141]**4 + 1e-10) + 1\n",
    "X_train[:,:,142] = 0.1 * np.log10(1e20*X_train[:,:,142]**4 + 1e-10) + 1\n",
    "X_train[:,:,143] = 0.1 * np.log10(1e20*X_train[:,:,143]**4 + 1e-10) + 1\n",
    "X_train[:,:,144] = 0.1 * np.log10(1e20*X_train[:,:,144]**10 + 1e-10) + 1\n",
    "X_train[:,:,146] = 0.1 * np.log10(1e20*X_train[:,:,146]**10 + 1e-10) + 1\n",
    "X_train[:,:,147] = 0.1 * np.log10(1e20*X_train[:,:,147]**9 + 1e-10) + 1\n",
    "X_train[:,:,148] = 0.1 * np.log10(1e20*X_train[:,:,148]**8.5 + 1e-10) + 1\n",
    "X_train[:,:,149] = 0.1 * np.log10(1e20*X_train[:,:,149]**9 + 1e-10) + 1\n",
    "X_train[:,:,150] = 0.1 * np.log10(1e20*X_train[:,:,150]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,151] = 0.1 * np.log10(1e20*X_train[:,:,151]**12 + 1e-10) + 1\n",
    "X_train[:,:,152] = 0.1 * np.log10(1e20*X_train[:,:,152]**20 + 1e-10) + 1\n",
    "X_train[:,:,153] = 0.1 * np.log10(1e20*X_train[:,:,153]**11 + 1e-10) + 1\n",
    "X_train[:,:,154] = 0.1 * np.log10(1e20*X_train[:,:,154]**20 + 1e-10) + 1\n",
    "X_train[:,:,155] = 0.1 * np.log10(1e20*X_train[:,:,155]**16 + 1e-10) + 1\n",
    "X_train[:,:,156] = 0.1 * np.log10(1e20*X_train[:,:,156]**10 + 1e-10) + 1\n",
    "X_train[:,:,157] = 0.1 * np.log10(1e20*X_train[:,:,157]**11 + 1e-10) + 1\n",
    "X_train[:,:,158] = 0.1 * np.log10(1e20*X_train[:,:,158]**9 + 1e-10) + 1\n",
    "X_train[:,:,159] = 0.1 * np.log10(1e20*X_train[:,:,159]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,160] = 0.1 * np.log10(1e20*X_train[:,:,160]**11 + 1e-10) + 1\n",
    "X_train[:,:,166] = 0.1 * np.log10(1e20*X_train[:,:,166]**11 + 1e-10) + 1\n",
    "X_train[:,:,168] = 0.1 * np.log10(1e20*X_train[:,:,168]**14 + 1e-10) + 1\n",
    "X_train[:,:,173] = 0.1 * np.log10(1e30*X_train[:,:,173]**7 + 1e-10) + 1\n",
    "X_train[:,:,174] = 0.1 * np.log10(1e30*X_train[:,:,174]**4 + 1e-10) + 1\n",
    "X_train[:,:,178] = 0.1 * np.log10(1e30*X_train[:,:,178]**4 + 1e-10) + 1\n",
    "X_train[:,:,179] = 0.1 * np.log10(1e30*X_train[:,:,179]**4 + 1e-10) + 1\n",
    "X_train[:,:,181] = 0.1 * np.log10(1e30*X_train[:,:,181]**7 + 1e-10) + 1\n",
    "X_train[:,:,182] = 0.1 * np.log10(1e30*X_train[:,:,182]**2 + 1e-10) + 1\n",
    "X_train[:,:,183] = 0.1 * np.log10(1e30*X_train[:,:,183]**2 + 1e-10) + 1\n",
    "X_train[:,:,185] = 0.1 * np.log10(1e30*X_train[:,:,185]**2 + 1e-10) + 1\n",
    "X_train[:,:,186] = 0.1 * np.log10(1e30*X_train[:,:,186]**2 + 1e-10) + 1\n",
    "X_train[:,:,187] = 0.1 * np.log10(1e30*X_train[:,:,187]**2 + 1e-10) + 1\n",
    "X_train[:,:,190] = 0.1 * np.log10(1e10*X_train[:,:,190]**4 + 1e-10) + 1\n",
    "X_train[:,:,191] = 0.1 * np.log10(1e20*X_train[:,:,191]**6 + 1e-10) + 1\n",
    "X_train[:,:,192] = 0.1 * np.log10(1e20*X_train[:,:,192]**4 + 1e-10) + 1\n",
    "X_train[:,:,195] = 0.1 * np.log10(1e20*X_train[:,:,195]**4 + 1e-10) + 1\n",
    "X_train[:,:,196] = 0.1 * np.log10(1e20*X_train[:,:,196]**6 + 1e-10) + 1\n",
    "X_train[:,:,197] = 0.1 * np.log10(1e20*X_train[:,:,197]**4 + 1e-10) + 1\n",
    "X_train[:,:,198] = 0.1 * np.log10(1e20*X_train[:,:,198]**6 + 1e-10) + 1\n",
    "X_train[:,:,200] = 0.1 * np.log10(1e20*X_train[:,:,200]**6 + 1e-10) + 1\n",
    "X_train[:,:,201] = 0.1 * np.log10(1e20*X_train[:,:,201]**5 + 1e-10) + 1\n",
    "X_train[:,:,202] = 0.1 * np.log10(1e20*X_train[:,:,202]**5 + 1e-10) + 1\n",
    "X_train[:,:,203] = 0.1 * np.log10(1e20*X_train[:,:,203]**5 + 1e-10) + 1\n",
    "X_train[:,:,204] = 0.1 * np.log10(1e30*X_train[:,:,204]**3 + 1e-10) + 1\n",
    "X_train[:,:,205] = 0.1 * np.log10(1e30*X_train[:,:,205]**3 + 1e-10) + 1\n",
    "X_train[:,:,206] = 0.1 * np.log10(1e20*X_train[:,:,206]**7 + 1e-10) + 1\n",
    "X_train[:,:,207] = 0.1 * np.log10(1e30*X_train[:,:,207]**2 + 1e-10) + 1\n",
    "X_train[:,:,208] = 0.1 * np.log10(1e30*X_train[:,:,208]**2 + 1e-10) + 1\n",
    "X_train[:,:,209] = 0.1 * np.log10(1e30*X_train[:,:,209]**2 + 1e-10) + 1\n",
    "X_train[:,:,213] = 0.1 * np.log10(1e30*X_train[:,:,213]**2 + 1e-10) + 1\n",
    "X_train[:,:,214] = 0.1 * np.log10(1e7*X_train[:,:,214]**2 + 1e-10) + 1\n",
    "X_train[:,:,215] = 0.1 * np.log10(1e15*X_train[:,:,215]**2 + 1e-10) + 1\n",
    "X_train[:,:,216] = 0.1 * np.log10(1e15*X_train[:,:,216]**2 + 1e-10) + 1\n",
    "X_train[:,:,217] = 0.1 * np.log10(1e15*X_train[:,:,217]**2 + 1e-10) + 1\n",
    "X_train[:,:,218] = 0.1 * np.log10(1e15*X_train[:,:,218]**2 + 1e-10) + 1\n",
    "X_train[:,:,220] = 0.1 * np.log10(1e15*X_train[:,:,220]**2 + 1e-10) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Feat\n",
    "X_train = np.dstack([X_train[:,:,:314], X_train[:,:,-14:]])\n",
    "X_test = np.dstack([X_test[:,:,:314], X_test[:,:,-14:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2355, 4, 328), (2250, 4, 328))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Numpy tensor into PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append((X_train[i], y_train[i]))\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append((X_test[i], y_test[i]))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = ConvNet(in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "\n",
    "# classifier = GRU(in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "# classifier.apply(weight_init)\n",
    "# focal_loss = FocalLoss2(alpha, gamma_pos, gamma_neg)\n",
    "# if optim == \"adam\":\n",
    "#     optim_clsfr = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), \n",
    "#                              lr=learn_rate)\n",
    "# else:\n",
    "#     optim_clsfr = RangerLars(filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "#                              lr=learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(pred_y_list, label_list, save_path=False, stat='Train'):\n",
    "    ###########################################################\n",
    "    plt.ylabel('Log Count')\n",
    "    plt.xlabel('Score')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['left'].set_color('none')\n",
    "    ax.spines['bottom'].set_color('none')\n",
    "    \n",
    "    MIN = 0.4\n",
    "    MAX = 0.6\n",
    "    BIN = 100\n",
    "    plt.grid(color = '#9999CC')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 0)], \n",
    "             bins=[n/(10*BIN) for n in range(int(10*MIN)*BIN, int(10*MAX)*BIN)], \n",
    "             label='Negative',\n",
    "             color='#598987')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 1)], \n",
    "             bins=[n/(10*BIN) for n in range(int(10*MIN)*BIN, int(10*MAX)*BIN)], \n",
    "             label='Positive', \n",
    "             color='#FFD000')\n",
    "    plt.legend(loc='upper right')\n",
    "    if save_path:\n",
    "        plt.savefig(\"result/{}_{}.jpg\".format(save_path, stat), dpi=1000, quality=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(PATH, save_path=False):\n",
    "    # optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr=learn_rate)\n",
    "\n",
    "    # model = GRU(in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "    model = ConvNet(fc_dims=[128, 64], in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    ##### Training #####\n",
    "    Training_label_list = []\n",
    "    Training_pred_y_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in tqdm_notebook(enumerate(train_dataloader)):\n",
    "        if data.size()[0] != train_dataloader.batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        # Update classifier\n",
    "\n",
    "        # pred_y = model(data).squeeze(-1)\n",
    "        # pred_y = torch.nn.functional.softmax(pred_y, dim=1)[:, 1]\n",
    "        \n",
    "        Training_pred_y = model(data.permute(0, 2, 1)).squeeze(-1)     # for Conv1D\n",
    "        Training_pred_y = torch.nn.functional.softmax(Training_pred_y, dim=1)[:, 1]\n",
    "\n",
    "        Training_label_list += list(target.cpu().detach().numpy())\n",
    "        Training_pred_y_list += list(Training_pred_y.cpu().detach().numpy())\n",
    "    \n",
    "    \n",
    "    Training_label_list = np.array(Training_label_list)\n",
    "    Training_pred_y_list = np.array(Training_pred_y_list)\n",
    "    auc_train = roc_auc_score(Training_label_list, Training_pred_y_list)\n",
    "    \n",
    "    ##### Testing #####\n",
    "    Testing_label_list = []\n",
    "    Testing_pred_y_list = []\n",
    "    for batch_idx, (data, target) in tqdm_notebook(enumerate(test_dataloader)):\n",
    "        if data.size()[0] != test_dataloader.batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        # Update classifier\n",
    "\n",
    "        # pred_y = model(data).squeeze(-1)\n",
    "        # pred_y = torch.nn.functional.softmax(pred_y, dim=1)[:, 1]\n",
    "        \n",
    "        Testing_pred_y = model(data.permute(0, 2, 1)).squeeze(-1)     # for Conv1D\n",
    "        Testing_pred_y = torch.nn.functional.softmax(Testing_pred_y, dim=1)[:, 1]\n",
    "\n",
    "        Testing_label_list += list(target.cpu().detach().numpy())\n",
    "        Testing_pred_y_list += list(Testing_pred_y.cpu().detach().numpy())\n",
    "    \n",
    "    Testing_label_list = np.array(Testing_label_list)\n",
    "    Testing_pred_y_list = np.array(Testing_pred_y_list)\n",
    "    auc_test = roc_auc_score(Testing_label_list, Testing_pred_y_list)\n",
    "    \n",
    "    ### Performance \n",
    "    ratio = .05\n",
    "    thres1 = np.min(Training_pred_y_list[np.where(Training_label_list == 1)])\n",
    "    thres2 = sorted(Testing_pred_y_list, reverse=True)[int(Testing_label_list.sum() + int(len(Testing_label_list)*ratio))]\n",
    "    \n",
    "    # thres2 = np.min(Testing_pred_y_list[np.where(y_test == 1)])\n",
    "    # thres3 = (8*thres1+2*thres2)/10 \n",
    "    # cut1, cut2 = thres1, thres2\n",
    "    \n",
    "    print ('Cut1: {}'.format(thres1))\n",
    "    print ('Cut2: {}'.format(thres2))\n",
    "\n",
    "    print(\"Recall of Training = 1 : {}\".format(thres1 > np.min(Training_pred_y_list)))\n",
    "    print('Training => auc: {:.6f}'.format(auc_train))\n",
    "    print (\"Training Treshold: {}\".format(thres1))\n",
    "    \n",
    "    y_predict_bin = np.array(Training_pred_y_list >= thres1, dtype=int)\n",
    "    prec_train, recall_train, f1_train = evaluate(Training_label_list, y_predict_bin)\n",
    "    print('prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.format(prec_train, recall_train, f1_train))\n",
    "    print (\"Total Positve: {}\".format(int(sum(Training_label_list))))\n",
    "    num_cand = np.sum(Training_pred_y_list >= thres1)\n",
    "    print (\"Total Candidate: {}\".format(num_cand))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    ## High Risk\n",
    "    print(\"High Risk Positve: {}\".format(np.sum(Training_label_list[Training_pred_y_list >= thres1])))\n",
    "    print(\"High Risk Candidate: {}\".format(np.sum(Training_pred_y_list >= thres1)))\n",
    "    print(\"High Risk Prec: {}\".format(np.sum(Training_label_list[np.where(Training_pred_y_list >= thres1)])/\\\n",
    "                                      np.sum(Training_pred_y_list >= thres1)))\n",
    "    print('----------------------------------------------')\n",
    "    ## Medium Risk\n",
    "    print(\"Medium Risk Positve: {}\".format(np.sum(Training_label_list[(Training_pred_y_list < thres1) &\\\n",
    "                                                                     (Training_pred_y_list >= thres2)])))\n",
    "    print(\"Medium Risk Candidate: {}\".format(np.sum((Training_pred_y_list < thres1) &\\\n",
    "                                                    (Training_pred_y_list >= thres2))))\n",
    "    print(\"Medium Risk Prec: {}\".format(np.sum(Training_label_list[np.where((Training_pred_y_list < thres1) &\\\n",
    "                                                                           (Training_pred_y_list >= thres2))])/\\\n",
    "                                                             np.sum((Training_pred_y_list < thres1) &\\\n",
    "                                                                    (Training_pred_y_list >= thres2))))\n",
    "    \n",
    "    plotting(Training_pred_y_list, Training_label_list, save_path)\n",
    "#     pd.DataFrame([Training_pred_y_list, Training_label_list, training_announce.tolist()], \n",
    "#                  index=['score', 'label', 'annouce']).T.to_csv('result/Training_result.csv', index=None)\n",
    "    ####ã€€----------------------------------------------\n",
    "    print(\"Recall of Testing = 1 : {}\".format(thres2 <= np.min(Testing_pred_y_list[Testing_label_list == 1])))\n",
    "    print('Testing => auc: {:.6f}'.format(auc_test))\n",
    "    print(\"Threshold is set to {}\".format(thres2))\n",
    "    \n",
    "    y_predict_bin = np.array(Testing_pred_y_list >= thres2, dtype=int)\n",
    "    prec_test, recall_test, f1_test = evaluate(Testing_label_list, y_predict_bin)\n",
    "    print('prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.format(prec_test, recall_test, f1_test))\n",
    "    print (\"Total Positve: {}\".format(int(sum(Testing_label_list))))\n",
    "    print(\"Total Candidate: {}\".format(np.sum(Testing_pred_y_list >= thres2)))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    ## High Risk\n",
    "    print(\"High Risk Positve: {}\".format(np.sum(Testing_label_list[Testing_pred_y_list >= thres1])))\n",
    "    print(\"High Risk Candidate: {}\".format(np.sum(Testing_pred_y_list >= thres1)))\n",
    "    print(\"High Risk Prec: {}\".format(np.sum(Testing_label_list[np.where(Testing_pred_y_list >= thres1)])/\\\n",
    "                                      np.sum(Testing_pred_y_list >= thres1)))\n",
    "    print('----------------------------------------------')\n",
    "    ## Medium Risk\n",
    "    print(\"Medium Risk Positve: {}\".format(np.sum(Testing_label_list[(Testing_pred_y_list < thres1) &\\\n",
    "                                                                     (Testing_pred_y_list >= thres2)])))\n",
    "    print(\"Medium Risk Candidate: {}\".format(np.sum((Testing_pred_y_list < thres1) &\\\n",
    "                                                    (Testing_pred_y_list >= thres2))))\n",
    "    print(\"Medium Risk Prec: {}\".format(np.sum(Testing_label_list[np.where((Testing_pred_y_list < thres1) &\\\n",
    "                                                                           (Testing_pred_y_list >= thres2))])/\\\n",
    "                                                             np.sum((Testing_pred_y_list < thres1) &\\\n",
    "                                                                    (Testing_pred_y_list >= thres2))))\n",
    "\n",
    "    \n",
    "    plotting(Testing_pred_y_list, Testing_label_list, save_path, stat='Test')\n",
    "#     pd.DataFrame([Testing_pred_y_list, Testing_label_list, testing_announce.tolist()], \n",
    "#                  index=['score', 'label', 'annouce']).T.to_csv('result/Testing_result.csv', index=None)\n",
    "    return Training_label_list, Training_pred_y_list, Testing_label_list, Testing_pred_y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_dim = [328, 656, 328]\n",
      "fc_dim = [328/0.9, 128/0.8, 64/0.8, 2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d6584749964973966176edf80d9436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5cbcca3e7345499ea158bf1d3e8005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cut1: 0.5233184695243835\n",
      "Cut2: 0.4935992956161499\n",
      "Recall of Training = 1 : True\n",
      "Training => auc: 1.000000\n",
      "Training Treshold: 0.5233184695243835\n",
      "prec: 1.0000, rec: 1.0000, F1score: 1.0000\n",
      "Total Positve: 102\n",
      "Total Candidate: 102\n",
      "----------------------------------------------\n",
      "High Risk Positve: 102.0\n",
      "High Risk Candidate: 102\n",
      "High Risk Prec: 1.0\n",
      "----------------------------------------------\n",
      "Medium Risk Positve: 0.0\n",
      "Medium Risk Candidate: 0\n",
      "Medium Risk Prec: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of Testing = 1 : True\n",
      "Testing => auc: 0.981448\n",
      "Threshold is set to 0.4935992956161499\n",
      "prec: 0.1374, rec: 1.0000, F1score: 0.2416\n",
      "Total Positve: 18\n",
      "Total Candidate: 131\n",
      "----------------------------------------------\n",
      "High Risk Positve: 5.0\n",
      "High Risk Candidate: 17\n",
      "High Risk Prec: 0.29411764705882354\n",
      "----------------------------------------------\n",
      "Medium Risk Positve: 13.0\n",
      "Medium Risk Candidate: 114\n",
      "Medium Risk Prec: 0.11403508771929824\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH = '../../user_data/CloudMile/data/saved_models/Feat-ts-Conv1Dsmall_v2_eps-21_focal62_rlars_lr4_epoch1600_Part2_0.2647058823529412'\n",
    "Training_label_list, Training_pred_y_list, Testing_label_list, Testing_pred_y_list= func(PATH, 'Feat-ts-Golden-LastSoftmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
