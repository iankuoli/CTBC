{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/aiuser03/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(fan_in, fan_out, constant=1): \n",
    "    \"\"\" Xavier initialization of network weights\"\"\"\n",
    "    # https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "    low = -constant*np.sqrt(6.0/(fan_in + fan_out)) \n",
    "    high = constant*np.sqrt(6.0/(fan_in + fan_out))\n",
    "    return tf.random_uniform((fan_in, fan_out), \n",
    "                             minval=low, maxval=high, \n",
    "                             dtype=tf.float32)\n",
    "\n",
    "class VariationalAutoencoder(object):\n",
    "    \"\"\" Variation Autoencoder (VAE) with an sklearn-like interface implemented using TensorFlow.\n",
    "    \n",
    "    This implementation uses probabilistic encoders and decoders using Gaussian \n",
    "    distributions and  realized by multi-layer perceptrons. The VAE can be learned\n",
    "    end-to-end.\n",
    "    \n",
    "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus, \n",
    "                 learning_rate=0.001, batch_size=100):\n",
    "        self.network_architecture = network_architecture\n",
    "        self.transfer_fct = transfer_fct\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # tf Graph input\n",
    "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]])\n",
    "        \n",
    "        # Create autoencoder network\n",
    "        self._create_network()\n",
    "        # Define loss function based variational upper-bound and \n",
    "        # corresponding optimizer\n",
    "        self._create_loss_optimizer()\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "    def _create_network(self):\n",
    "        # Initialize autoencode network weights and biases\n",
    "        network_weights = self._initialize_weights(**self.network_architecture)\n",
    "\n",
    "        # Use recognition network to determine mean and \n",
    "        # (log) variance of Gaussian distribution in latent\n",
    "        # space\n",
    "        self.z_mean, self.z_log_sigma_sq = \\\n",
    "            self._recognition_network(network_weights[\"weights_recog\"], \n",
    "                                      network_weights[\"biases_recog\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution\n",
    "        n_z = self.network_architecture[\"n_z\"]\n",
    "        eps = tf.random_normal((self.batch_size, n_z), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma*epsilon\n",
    "        self.z = tf.add(self.z_mean, \n",
    "                        tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        # Use generator to determine mean of\n",
    "        # Bernoulli distribution of reconstructed input\n",
    "        self.x_reconstr_mean = \\\n",
    "            self._generator_network(network_weights[\"weights_gener\"],\n",
    "                                    network_weights[\"biases_gener\"])\n",
    "            \n",
    "    def _initialize_weights(self, n_hidden_recog_1, n_hidden_recog_2, \n",
    "                            n_hidden_gener_1,  n_hidden_gener_2, \n",
    "                            n_input, n_z):\n",
    "        all_weights = dict()\n",
    "        all_weights['weights_recog'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_input, n_hidden_recog_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_recog_1, n_hidden_recog_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_recog_2, n_z)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_recog_2, n_z))}\n",
    "        all_weights['biases_recog'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_recog_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_recog_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_z], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_z], dtype=tf.float32))}\n",
    "        all_weights['weights_gener'] = {\n",
    "            'h1': tf.Variable(xavier_init(n_z, n_hidden_gener_1)),\n",
    "            'h2': tf.Variable(xavier_init(n_hidden_gener_1, n_hidden_gener_2)),\n",
    "            'out_mean': tf.Variable(xavier_init(n_hidden_gener_2, n_input)),\n",
    "            'out_log_sigma': tf.Variable(xavier_init(n_hidden_gener_2, n_input))}\n",
    "        all_weights['biases_gener'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_gener_1], dtype=tf.float32)),\n",
    "            'b2': tf.Variable(tf.zeros([n_hidden_gener_2], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([n_input], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(tf.zeros([n_input], dtype=tf.float32))}\n",
    "        return all_weights\n",
    "            \n",
    "    def _recognition_network(self, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network), which\n",
    "        # maps inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.x, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        z_mean = tf.add(tf.matmul(layer_2, weights['out_mean']),\n",
    "                        biases['out_mean'])\n",
    "        z_log_sigma_sq = \\\n",
    "            tf.add(tf.matmul(layer_2, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma'])\n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network), which\n",
    "        # maps points in latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer_1 = self.transfer_fct(tf.add(tf.matmul(self.z, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        layer_2 = self.transfer_fct(tf.add(tf.matmul(layer_1, weights['h2']), \n",
    "                                           biases['b2'])) \n",
    "        x_reconstr_mean = \\\n",
    "            tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['out_mean']), \n",
    "                                 biases['out_mean']))\n",
    "        return x_reconstr_mean\n",
    "            \n",
    "    def _create_loss_optimizer(self):\n",
    "        # The loss is composed of two terms:\n",
    "        # 1.) The reconstruction loss (the negative log probability\n",
    "        #     of the input under the reconstructed Bernoulli distribution \n",
    "        #     induced by the decoder in the data space).\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for reconstructing the input when the activation in latent\n",
    "        #     is given.\n",
    "        # Adding 1e-10 to avoid evaluation of log(0.0)\n",
    "        reconstr_loss = \\\n",
    "            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
    "                           + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean),\n",
    "                           1)\n",
    "        # 2.) The latent loss, which is defined as the Kullback Leibler divergence \n",
    "        ##    between the distribution in latent space induced by the encoder on \n",
    "        #     the data and some prior. This acts as a kind of regularizer.\n",
    "        #     This can be interpreted as the number of \"nats\" required\n",
    "        #     for transmitting the the latent space distribution given\n",
    "        #     the prior.\n",
    "        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq \n",
    "                                           - tf.square(self.z_mean) \n",
    "                                           - tf.exp(self.z_log_sigma_sq), 1)\n",
    "        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   # average over batch\n",
    "        # Use ADAM optimizer\n",
    "        self.optimizer = \\\n",
    "            tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "        \n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                  feed_dict={self.x: X})\n",
    "        return cost\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "    \n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.network_architecture[\"n_z\"])\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu})\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = glob.glob('./Edge_Attribute/*.csv')\n",
    "dfs = [pd.read_csv(url) for url in urls]\n",
    "df = pd.concat(dfs)\n",
    "AML_dataset = df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Large_TransOut_Count</th>\n",
       "      <th>TransOut_Count</th>\n",
       "      <th>Total_Large_TransIn</th>\n",
       "      <th>Total_WireTrans</th>\n",
       "      <th>Total_WireTrans_Times</th>\n",
       "      <th>Average_WireTrans</th>\n",
       "      <th>WireTransIn_8000</th>\n",
       "      <th>WireTrans_Out_9mon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Large_TransOut_Count  TransOut_Count  Total_Large_TransIn  Total_WireTrans  \\\n",
       "0              0.000000        0.000000             0.000282         0.000000   \n",
       "1              0.006369        0.006369             0.000000         0.000164   \n",
       "2              0.006369        0.006369             0.000000         0.000749   \n",
       "3              0.000000        0.000000             0.000000         0.000000   \n",
       "4              0.000000        0.000000             0.000000         0.000000   \n",
       "\n",
       "   Total_WireTrans_Times  Average_WireTrans  WireTransIn_8000  \\\n",
       "0               0.000000           0.000000               0.0   \n",
       "1               0.006369           0.000164               0.0   \n",
       "2               0.006369           0.000749               0.0   \n",
       "3               0.000000           0.000000               0.0   \n",
       "4               0.000000           0.000000               0.0   \n",
       "\n",
       "   WireTrans_Out_9mon  \n",
       "0            0.000000  \n",
       "1            0.000114  \n",
       "2            0.001714  \n",
       "3            0.000213  \n",
       "4            0.000263  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "features = scaler.fit_transform(AML_dataset)\n",
    "AML_dataset = pd.DataFrame(features, columns=AML_dataset.columns, index=AML_dataset.index)\n",
    "AML_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network_architecture, learning_rate=1e-5,\n",
    "          batch_size=100, training_epochs=10, display_step=5):\n",
    "    vae = VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(AML_dataset.shape[0] / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs = AML_dataset.sample(batch_size).values\n",
    "\n",
    "            # Fit training using batch data\n",
    "            cost = vae.partial_fit(batch_xs)\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / AML_dataset.shape[0] * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        # if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "              \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/DL/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 7.163430833\n",
      "Epoch: 0002 cost= 2.304462268\n",
      "Epoch: 0003 cost= 0.544516757\n",
      "Epoch: 0004 cost= 0.127194526\n",
      "Epoch: 0005 cost= 0.098249826\n",
      "Epoch: 0006 cost= 0.096311066\n",
      "Epoch: 0007 cost= 0.096185981\n",
      "Epoch: 0008 cost= 0.095685223\n",
      "Epoch: 0009 cost= 0.095280403\n",
      "Epoch: 0010 cost= 0.095814656\n",
      "Epoch: 0011 cost= 0.095731368\n",
      "Epoch: 0012 cost= 0.095556773\n",
      "Epoch: 0013 cost= 0.095315053\n",
      "Epoch: 0014 cost= 0.095364627\n",
      "Epoch: 0015 cost= 0.095330970\n",
      "Epoch: 0016 cost= 0.094918203\n",
      "Epoch: 0017 cost= 0.095057792\n",
      "Epoch: 0018 cost= 0.095067412\n",
      "Epoch: 0019 cost= 0.094961975\n",
      "Epoch: 0020 cost= 0.095150083\n",
      "Epoch: 0021 cost= 0.094959954\n",
      "Epoch: 0022 cost= 0.095099701\n",
      "Epoch: 0023 cost= 0.095234490\n",
      "Epoch: 0024 cost= 0.095312442\n",
      "Epoch: 0025 cost= 0.094758728\n",
      "Epoch: 0026 cost= 0.095169129\n",
      "Epoch: 0027 cost= 0.095222453\n",
      "Epoch: 0028 cost= 0.094939535\n",
      "Epoch: 0029 cost= 0.095112696\n",
      "Epoch: 0030 cost= 0.095235376\n",
      "Epoch: 0031 cost= 0.094471236\n",
      "Epoch: 0032 cost= 0.095102230\n",
      "Epoch: 0033 cost= 0.095296103\n",
      "Epoch: 0034 cost= 0.094987118\n",
      "Epoch: 0035 cost= 0.095086050\n",
      "Epoch: 0036 cost= 0.094560074\n",
      "Epoch: 0037 cost= 0.094877452\n",
      "Epoch: 0038 cost= 0.094643166\n",
      "Epoch: 0039 cost= 0.094621916\n",
      "Epoch: 0040 cost= 0.094196272\n",
      "Epoch: 0041 cost= 0.094988528\n",
      "Epoch: 0042 cost= 0.094902994\n",
      "Epoch: 0043 cost= 0.094551026\n",
      "Epoch: 0044 cost= 0.094861326\n",
      "Epoch: 0045 cost= 0.094932365\n",
      "Epoch: 0046 cost= 0.094860139\n",
      "Epoch: 0047 cost= 0.094921411\n",
      "Epoch: 0048 cost= 0.094717752\n",
      "Epoch: 0049 cost= 0.094612397\n",
      "Epoch: 0050 cost= 0.094237507\n",
      "Epoch: 0051 cost= 0.095049739\n",
      "Epoch: 0052 cost= 0.094306048\n",
      "Epoch: 0053 cost= 0.094888311\n",
      "Epoch: 0054 cost= 0.094859259\n",
      "Epoch: 0055 cost= 0.094900519\n",
      "Epoch: 0056 cost= 0.095053046\n",
      "Epoch: 0057 cost= 0.094934063\n",
      "Epoch: 0058 cost= 0.094785283\n",
      "Epoch: 0059 cost= 0.094677952\n",
      "Epoch: 0060 cost= 0.095072415\n",
      "Epoch: 0061 cost= 0.094579622\n",
      "Epoch: 0062 cost= 0.095073337\n",
      "Epoch: 0063 cost= 0.094412844\n",
      "Epoch: 0064 cost= 0.094314897\n",
      "Epoch: 0065 cost= 0.094758331\n",
      "Epoch: 0066 cost= 0.094594847\n",
      "Epoch: 0067 cost= 0.094511270\n",
      "Epoch: 0068 cost= 0.095164751\n",
      "Epoch: 0069 cost= 0.095023982\n",
      "Epoch: 0070 cost= 0.094546397\n",
      "Epoch: 0071 cost= 0.094649236\n",
      "Epoch: 0072 cost= 0.094777878\n",
      "Epoch: 0073 cost= 0.095192519\n",
      "Epoch: 0074 cost= 0.094445756\n",
      "Epoch: 0075 cost= 0.094813165\n",
      "Epoch: 0076 cost= 0.095005237\n",
      "Epoch: 0077 cost= 0.095112133\n",
      "Epoch: 0078 cost= 0.094908201\n",
      "Epoch: 0079 cost= 0.094886633\n",
      "Epoch: 0080 cost= 0.094869011\n",
      "Epoch: 0081 cost= 0.094811855\n",
      "Epoch: 0082 cost= 0.094610033\n",
      "Epoch: 0083 cost= 0.094914663\n",
      "Epoch: 0084 cost= 0.094289229\n",
      "Epoch: 0085 cost= 0.094358743\n",
      "Epoch: 0086 cost= 0.095020277\n",
      "Epoch: 0087 cost= 0.094480422\n",
      "Epoch: 0088 cost= 0.094738059\n",
      "Epoch: 0089 cost= 0.094648162\n",
      "Epoch: 0090 cost= 0.094664000\n",
      "Epoch: 0091 cost= 0.095338153\n",
      "Epoch: 0092 cost= 0.094594898\n",
      "Epoch: 0093 cost= 0.094681189\n",
      "Epoch: 0094 cost= 0.094277088\n",
      "Epoch: 0095 cost= 0.094695273\n",
      "Epoch: 0096 cost= 0.095036756\n",
      "Epoch: 0097 cost= 0.094183997\n",
      "Epoch: 0098 cost= 0.094606516\n",
      "Epoch: 0099 cost= 0.095102969\n",
      "Epoch: 0100 cost= 0.094878910\n",
      "Epoch: 0101 cost= 0.095105305\n",
      "Epoch: 0102 cost= 0.094922021\n",
      "Epoch: 0103 cost= 0.094974198\n",
      "Epoch: 0104 cost= 0.094619782\n",
      "Epoch: 0105 cost= 0.094630517\n",
      "Epoch: 0106 cost= 0.094320024\n",
      "Epoch: 0107 cost= 0.094508919\n",
      "Epoch: 0108 cost= 0.094707253\n",
      "Epoch: 0109 cost= 0.094849242\n",
      "Epoch: 0110 cost= 0.094505371\n",
      "Epoch: 0111 cost= 0.094474112\n",
      "Epoch: 0112 cost= 0.094953783\n",
      "Epoch: 0113 cost= 0.094721835\n",
      "Epoch: 0114 cost= 0.094994614\n",
      "Epoch: 0115 cost= 0.094322311\n",
      "Epoch: 0116 cost= 0.094722103\n",
      "Epoch: 0117 cost= 0.094371767\n",
      "Epoch: 0118 cost= 0.094948449\n",
      "Epoch: 0119 cost= 0.095017023\n",
      "Epoch: 0120 cost= 0.094731711\n",
      "Epoch: 0121 cost= 0.094622089\n",
      "Epoch: 0122 cost= 0.094961671\n",
      "Epoch: 0123 cost= 0.094787309\n",
      "Epoch: 0124 cost= 0.094624850\n",
      "Epoch: 0125 cost= 0.094400183\n",
      "Epoch: 0126 cost= 0.095076144\n",
      "Epoch: 0127 cost= 0.094643272\n",
      "Epoch: 0128 cost= 0.094562378\n",
      "Epoch: 0129 cost= 0.094761147\n",
      "Epoch: 0130 cost= 0.094974224\n",
      "Epoch: 0131 cost= 0.094946233\n",
      "Epoch: 0132 cost= 0.095098929\n",
      "Epoch: 0133 cost= 0.094918654\n",
      "Epoch: 0134 cost= 0.094768235\n",
      "Epoch: 0135 cost= 0.094794736\n",
      "Epoch: 0136 cost= 0.094738735\n",
      "Epoch: 0137 cost= 0.094918709\n",
      "Epoch: 0138 cost= 0.095252924\n",
      "Epoch: 0139 cost= 0.095188795\n",
      "Epoch: 0140 cost= 0.094924901\n",
      "Epoch: 0141 cost= 0.095691598\n"
     ]
    }
   ],
   "source": [
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=6, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=5, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=5, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=6, # 2nd layer decoder neurons\n",
    "         n_input=AML_dataset.shape[1], # MNIST data input (img shape: 28*28)\n",
    "         n_z=3)  # dimensionality of latent space\n",
    "\n",
    "vae = train(network_architecture, training_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
