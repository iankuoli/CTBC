{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "from multiprocessing import Pool\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from spectral_normalization import SpectralNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weight_init import weight_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SN-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=256, z_dim=128):\n",
    "        super(VarEncoder, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.outdim_en1 = in_dim\n",
    "        self.outdim_en2 = math.ceil(self.outdim_en1 / 2)\n",
    "        self.dim_z = z_dim\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_dim, out_channels=in_dim*2, kernel_size=2),\n",
    "            nn.Conv1d(in_channels=in_dim*2, out_channels=in_dim*4, kernel_size=2),\n",
    "            nn.view(-1, in_dim*4),\n",
    "            nn.Linear(in_features=in_dim*4, out_features=self.outdim_en1),\n",
    "            nn.BatchNorm1d(self.outdim_en1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(in_features=outdim_en1, out_features=self.outdim_en2),\n",
    "            nn.BatchNorm1d(self.outdim_en2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.fc_zmean = nn.Linear(in_features=self.outdim_en2, out_features=self.dim_z)\n",
    "        self.fc_zvar = nn.Linear(in_features=self.outdim_en2, out_features=self.dim_z)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        return self.fc_zmean(h), self.fc_zvar(h)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=128, out_dim=256):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dim_z = z_dim\n",
    "        self.outdim_de1 = math.ceil(self.dim_z * 2)\n",
    "        self.outdim_de2 = math.ceil(self.outdim_de1 * 4)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=self.dim_z, out_features=self.outdim_de1),\n",
    "            nn.BatchNorm1d(self.outdim_de1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=outdim_de1, out_features=self.outdim_de2),\n",
    "            nn.BatchNorm1d(self.outdim_de2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.view(-1, 1, outdim_de2),\n",
    "            nn.Conv1d(in_channels=self.outdim_de2, out_channels=out_dim*2, kernel_size=2),\n",
    "            nn.Conv1d(in_channels=out_dim*2, out_channels=out_dim, kernel_size=2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128, data_dim=256, data_length=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.dim_z = z_dim\n",
    "        self.dim_data = data_dim\n",
    "        self.length_data = data_length\n",
    "        \n",
    "        self.enc_model = VarEncoder(in_dim=self.dim_data, z_dim=self.dim_z)\n",
    "        self.dec_model = Decoder(z_dim=self.dim_z, out_dim=self.dim_data)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.enc_model(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.dec_model(z)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def forward(self, z):\n",
    "        mu, logvar = self.encode(x.view(-1, data_dim, self.length_data))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim=256, data_length=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.dim_data = data_dim\n",
    "        self.outdim1 = self.dim_data\n",
    "        self.outdim2 = math.ceil(self.dim_data / 2)\n",
    "        self.length_data = data_length\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            SpectralNorm(nn.Conv1d(in_channels=in_dim, out_channels=in_dim*2, kernel_size=2)),\n",
    "            SpectralNorm(nn.Conv1d(in_channels=in_dim*2, out_channels=in_dim*4, kernel_size=2)),\n",
    "            nn.view(-1, in_dim*4),\n",
    "            SpectralNorm(nn.Linear(in_features=in_dim*4, out_features=self.outdim1)),\n",
    "            SpectralNorm(nn.BatchNorm1d(self.outdim1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            SpectralNorm(nn.Linear(in_features=outdim1, out_features=self.outdim2)),\n",
    "            SpectralNorm(nn.BatchNorm1d(self.outdim2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            SpectralNorm(nn.Linear(in_features=self.outdim2, out_features=1)),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x.view(-1, self.dim_data, self.length_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.01, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            F_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.emb_size = 661\n",
    "        self.hid1 = 200\n",
    "        self.hid2 = 100\n",
    "        self.rnn = nn.GRU(self.emb_size, num_layer=3, \n",
    "                          bidirectional=True, batch_first=True, dropout=0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.out1 = nn.Sequential(\n",
    "            nn.Linear(2*self.hid1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.4),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rnn, hid = self.rnn(src)\n",
    "        return self.out1(self.relu(rnn[:, -1]))\n",
    "        \n",
    "    def get_trainable_parameters(self):\n",
    "        return (param for param in self.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# GRU\n",
    "# ---------------------\n",
    "## focal loss\n",
    "alpha = 1e-4\n",
    "gamma = 2\n",
    "learn_rate = 1e-4\n",
    "\n",
    "train_batch_size = 32\n",
    "test_batch_size = 128\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "gan_loss = 'wasserstein'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../GPUArray_and_label.npz', allow_pickle=True)\n",
    "\n",
    "GPUArray = data['arr_0']\n",
    "label = data['arr_1']\n",
    "\n",
    "GPUArray = GPUArray[-1033905:,:,:]\n",
    "label = label[-1033905:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_set_split(GPUArray, label, random_state=42)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append((X_train[i], y_train[i]))\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append((X_test[i], y_test[i]))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of GAN + GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# GRU\n",
    "# ------------------------------------------------------------------------------------\n",
    "classifier = GRU().cuda()\n",
    "classifier.apply(weight_init)\n",
    "\n",
    "focal_loss = FocalLoss(alpha, gamma)\n",
    "optim_clsfr = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), \n",
    "                         lr=learn_rate)\n",
    "\n",
    "#\n",
    "# Encoder\n",
    "# ------------------------------------------------------------------------------------\n",
    "encoder = VarEncoder(in_dim=X_train.shape[2], z_dim=latent_dim).cuda()\n",
    "optim_enc = optim.Adam(filter(lambda p: p.requires_grad, encoder.parameters()), \n",
    "                       lr=learn_rate)\n",
    "\n",
    "#\n",
    "# GAN\n",
    "# ------------------------------------------------------------------------------------\n",
    "discriminator = Discriminator(data_dim=X_train.shape[2], \n",
    "                              data_length=X_train.shape[1]).cuda()\n",
    "generator = Generator(z_dim=latent_dim, \n",
    "                      data_dim=X_train.shape[2], \n",
    "                      data_length=X_train.shape[1]).cuda()\n",
    "\n",
    "optim_disc = optim.Adam(filter(lambda p: p.requires_grad, discriminator.parameters()), \n",
    "                        lr=learn_rate, betas=(0.0, 0.9))\n",
    "optim_gen = optim.Adam(filter(lambda p: p.requires_grad, generator.parameters()), \n",
    "                        lr=learn_rate, betas=(0.0, 0.9))\n",
    "\n",
    "scheduler_d = optim.lr_scheduler.ExponentialLR(optim_disc, gamma=0.99)\n",
    "scheduler_g = optim.lr_scheduler.ExponentialLR(optim_gen, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_iters = 2\n",
    "def train(epoch):\n",
    "    label_list = []\n",
    "    pred_y_list = []\n",
    "    \n",
    "    total_loss_batch = []\n",
    "    disc_loss_batch = []\n",
    "    gen_loss_batch = []\n",
    "    con_loss_batch = []\n",
    "    enc_loss_batch = []\n",
    "    clsf_loss_batch = []\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        if data.size()[0] != train_batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        \n",
    "        # Update discriminator\n",
    "        for _ in range(disc_iters):\n",
    "            optim_disc.zero_grad()\n",
    "            optim_gen.zero_grad()\n",
    "            \n",
    "            fake_data, mu, logvar = generator(data.permute(0, 2, 1))\n",
    "            \n",
    "            if gan_loss == 'hinge':\n",
    "                disc_loss = nn.ReLU()(1.0 - discriminator(data)).mean() + \\\n",
    "                            nn.ReLU()(1.0 + discriminator(fake_data)).mean()\n",
    "            elif gan_loss == 'wasserstein':\n",
    "                disc_loss = -discriminator(data).mean() + discriminator(fake_data).mean()\n",
    "            else:\n",
    "                disc_loss = nn.BCEWithLogitsLoss()(discriminator(data),\n",
    "                                                   Variable(torch.ones(train_batch_size,1).cuda())) + \\\n",
    "                            nn.BCEWithLogitsLoss()(discriminator(fake_data),\n",
    "                                                   Variable(torch.ones(train_batch_size,1).cuda()))\n",
    "            disc_loss.backward()\n",
    "            optim_disc.step()\n",
    "        \n",
    "        # Update generator, encoder, and classifier\n",
    "        optim_disc.zero_grad()\n",
    "        optim_gen.zero_grad()\n",
    "        optim_enc.zero_grad()\n",
    "        optim_clsfr.zero_grad()\n",
    "        \n",
    "        fake_data, mu, logvar = generator(data.permute(0, 2, 1))\n",
    "        fake_mu, fake_logvar = encoder(fake_data)\n",
    "        pred_y = classifier(torch.cat((data, data - fake_data.permute(0, 2, 1)), 2)).squeeze(-1)\n",
    "        \n",
    "        if gan_loss == 'hinge' or gan_loss == 'wasserstein':\n",
    "            gen_loss = -discriminator(fake_data).mean()\n",
    "        else:\n",
    "            gen_loss = nn.BCEWithLogitsLoss()(discriminator(fake_data),\n",
    "                                              Variable(torch.ones(train_batch_size,1).cuda()))\n",
    "        \n",
    "        con_loss = nn.L1Loss()(fake_data, data)\n",
    "        enc_loss = nn.MSELoss()(generator.reparameterize(mu, logvar),\n",
    "                                generator.reparameterize(fake_mu, fake_logvar))\n",
    "        clsf_loss = focal_loss(pred_y, target)\n",
    "        total_loss = gen_loss + con_loss + enc_loss + clsf_loss\n",
    "        total_loss.backward()\n",
    "        \n",
    "        optim_gen.step()\n",
    "        optim_enc.step()\n",
    "        optim_clsfr.step()\n",
    "        \n",
    "        total_loss_batch.append(total_loss)\n",
    "        disc_loss_batch.append(disc_loss)\n",
    "        gen_loss_batch.append(gen_loss)\n",
    "        enc_loss_batch.append(enc_loss)\n",
    "        con_loss_batch.append(con_loss)\n",
    "        clsf_loss_batch.append(clsf_loss)\n",
    "        \n",
    "        label_list += list(target.cpu().detach().numpy())\n",
    "        pred_y_list += list(pred_y.cpu().detach().numpy())\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('  Idx {} => disc: {:.4f}, gen: {:.4f}, con: {:.4f}, enc: {:.4f}, clsf: {:.4f}'.\n",
    "                  format(batch_idx, disc_loss, gen_loss, con_loss, enc_loss, clsf_loss))\n",
    "        \n",
    "    \n",
    "    scheduler_d.step()\n",
    "    scheduler_g.step()\n",
    "    \n",
    "    total_loss_avg = sum(total_loss_batch) / len(total_loss_batch)\n",
    "    disc_loss_avg = sum(disc_loss_batch) / len(disc_loss_batch)\n",
    "    gen_loss_avg = sum(gen_loss_batch) / len(gen_loss_batch)\n",
    "    con_loss_avg = sum(con_loss_batch) / len(con_loss_batch)\n",
    "    enc_loss_avg = sum(enc_loss_batch) / len(enc_loss_batch)\n",
    "    clsf_loss_avg = sum(clsf_loss_batch) / len(clsf_loss_batch)\n",
    "    \n",
    "    loss_tuple = (total_loss_avg, disc_loss_avg, gen_loss_avg, \n",
    "                  con_loss_avg, enc_loss_avg, clsf_loss_avg)\n",
    "    \n",
    "    return  label_list, pred_y_list, loss_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history_loss = []\n",
    "train_history_auc = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    discriminator.train(mode=True)\n",
    "    generator.train(mode=True)\n",
    "    encoder.train(mode=True)\n",
    "    classifier.train(mode=True)\n",
    "    \n",
    "    label_list, pred_y_list, loss_tuple = train(epoch)\n",
    "    \n",
    "    auc = roc_auc_score(label_list, pred_y_list)\n",
    "    train_history_loss.append(loss_tuple)\n",
    "    train_history_auc.append(auc)\n",
    "    \n",
    "    print('Epoch {} => auc:{}, total: {}, clsf: {}, disc: {}, gen: {}'.\n",
    "          format(epoch, auc, loss_tuple[0], loss_tuple[-1], loss_tuple[1], loss_tuple[2]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
