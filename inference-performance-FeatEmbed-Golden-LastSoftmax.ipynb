{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Classifier\n",
    "# ---------------------\n",
    "## focal loss\n",
    "alpha = 1\n",
    "gamma_pos = 6\n",
    "gamma_neg = 2\n",
    "grad_clip = 1\n",
    "lambda_l1 = 0\n",
    "weight_decay = 0   # lambda_l2\n",
    "\n",
    "#\n",
    "# VAT\n",
    "# ---------------------\n",
    "vat_xi = 1e-6\n",
    "vat_eps_pos = 1e2\n",
    "vat_eps_neg = 1e-1\n",
    "vat_ip = 1\n",
    "\n",
    "#\n",
    "# Training process\n",
    "# ---------------------\n",
    "train_batch_size = 128\n",
    "test_batch_size = 32\n",
    "\n",
    "#\n",
    "# Optimizer\n",
    "# ---------------------\n",
    "optim_type = 'rlars'       # ['adam', 'rlars']\n",
    "learn_rate = 1e-4\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "max_epochs = 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, fc_dims, in_dim=256, out_dim=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.conv1_dim = math.ceil(self.in_dim)\n",
    "        self.conv2_dim = math.ceil(self.in_dim * 2)\n",
    "        self.conv3_dim = math.ceil(self.in_dim)\n",
    "        self.outdim_en1 = fc_dims[0]\n",
    "        self.outdim_en2 = fc_dims[1]\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.model_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_dim, out_channels=self.conv1_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv1_dim, out_channels=self.conv2_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv2_dim, out_channels=self.conv3_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv3_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dropout_1 = 1 - 0.1\n",
    "        dropout_2 = 1 - 0.2\n",
    "        dropout_3 = 1 - 0.2\n",
    "        self.model_fc = nn.Sequential(\n",
    "            # FC 1\n",
    "            nn.Dropout(dropout_1),\n",
    "            nn.Linear(in_features=self.conv3_dim, out_features=self.outdim_en1),\n",
    "            nn.BatchNorm1d(self.outdim_en1),\n",
    "            nn.ReLU(),\n",
    "            # FC 2\n",
    "            nn.Dropout(dropout_2),\n",
    "            nn.Linear(in_features=self.outdim_en1, out_features=self.outdim_en2),\n",
    "            nn.BatchNorm1d(self.outdim_en2),\n",
    "            nn.ReLU(),\n",
    "            # FC 3\n",
    "            nn.Dropout(dropout_3),\n",
    "            nn.Linear(in_features=self.outdim_en2, out_features=self.out_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "        print(\"conv_dim = [{}, {}, {}]\".format(self.conv1_dim, self.conv2_dim, self.conv3_dim))\n",
    "        print(\"fc_dim = [{}/{}, {}/{}, {}/{}, {}]\".format(self.conv3_dim, dropout_1, \n",
    "                                                          self.outdim_en1, dropout_2, \n",
    "                                                          self.outdim_en2, dropout_3, self.out_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model_conv(x)\n",
    "        x = self.model_fc(x.view(x.shape[0], -1))\n",
    "        return x\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        return (param for param in self.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss2(nn.Module):\n",
    "    def __init__(self, alpha=0.01, gamma_pos=3, gamma_neg=2, logits=False, reduce=True):\n",
    "        super(FocalLoss2, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        gamma_diff = self.gamma_pos - self.gamma_neg\n",
    "        F_loss_pos = self.alpha * targets * (1-pt)**self.gamma_pos * BCE_loss\n",
    "        F_loss_pos = torch.mean(pt)**(-gamma_diff) * F_loss_pos\n",
    "        F_loss_neg = self.alpha * (1 - targets) * (1-pt)**self.gamma_neg * BCE_loss\n",
    "        F_loss = 1 * F_loss_pos + 0.9 * F_loss_neg\n",
    "        \n",
    "        avg_F_loss_pos = torch.sum(F_loss_pos) / torch.sum(targets)\n",
    "        avg_F_loss_neg = torch.sum(F_loss_neg) / torch.sum(1-targets)\n",
    "        \n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss), avg_F_loss_pos, avg_F_loss_neg\n",
    "        else:\n",
    "            return F_loss, F_loss_pos, F_loss_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Testing partition setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_date  = [(2018, 7), (2018, 8), (2018, 9),(2018, 10), (2018, 11), (2018, 12)]\n",
    "testing_date   = [(2019, 1), (2019, 2), (2019, 3), (2019, 4), (2019, 5), (2019, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(map(lambda ym: (ym, \n",
    "                            np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(*ym), allow_pickle=True)), \n",
    "                training_date + testing_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(map(lambda ym: data[ym], training_date))\n",
    "test_data = list(map(lambda ym: data[ym], testing_date))\n",
    "\n",
    "X_train_ = np.concatenate([data['arr_0'] for data in train_data])\n",
    "y_train_ = np.concatenate([data['arr_1'] for data in train_data])\n",
    "training_announce = np.concatenate([data['arr_2'] for data in train_data])\n",
    "training_FILTER = np.concatenate([data['arr_3'] for data in train_data])\n",
    "\n",
    "X_test_ = np.concatenate([data['arr_0'] for data in test_data])\n",
    "y_test_ = np.concatenate([data['arr_1'] for data in test_data])\n",
    "testing_announce = np.concatenate([data['arr_2'] for data in test_data])\n",
    "testing_FILTER = np.concatenate([data['arr_3'] for data in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting announced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_[training_announce == 1]\n",
    "y_train = y_train_[training_announce == 1]\n",
    "\n",
    "X_test = X_test_[testing_announce == 1]\n",
    "y_test = y_test_[testing_announce == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magical rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,:,0] = 0.1 * np.log10(1e4*X_train[:,:,0]**3 + 1e-10) + 1\n",
    "X_train[:,:,2] = 0.1 * np.log10(1e4*X_train[:,:,2]**5 + 1e-10) + 1\n",
    "X_train[:,:,3] = 0.1 * np.log10(1e4*X_train[:,:,3]**2 + 1e-10) + 1\n",
    "X_train[:,:,6] = 0.1 * np.log10(1e0*X_train[:,:,6]**3 + 1e-10) + 1\n",
    "X_train[:,:,7] = 0.1 * np.log10(1e8*X_train[:,:,7]**5 + 1e-10) + 1\n",
    "X_train[:,:,8] = 0.1 * np.log10(1e8*X_train[:,:,8]**3 + 1e-10) + 1\n",
    "X_train[:,:,9] = 0.1 * np.log10(1e8*X_train[:,:,9]**6 + 1e-10) + 1\n",
    "X_train[:,:,10] = 0.1 * np.log10(1e8*X_train[:,:,10]**2 + 1e-10) + 1\n",
    "X_train[:,:,12] = 0.1 * np.log10(1e9*X_train[:,:,12]**3 + 1e-10) + 1\n",
    "X_train[:,:,13] = 0.1 * np.log10(1e9*X_train[:,:,13]**5 + 1e-10) + 1\n",
    "X_train[:,:,14] = 0.1 * np.log10(1e9*X_train[:,:,14]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,15] = 0.1 * np.log10(1e9*X_train[:,:,15]**6 + 1e-10) + 1\n",
    "X_train[:,:,16] = 0.1 * np.log10(1e15*X_train[:,:,16]**3 + 1e-10) + 1\n",
    "X_train[:,:,18] = 0.1 * np.log10(1e15*X_train[:,:,18]**4 + 1e-10) + 1\n",
    "X_train[:,:,20] = 0.1 * np.log10(1e10*X_train[:,:,20]**8 + 1e-10) + 1\n",
    "X_train[:,:,21] = 0.1 * np.log10(1e8*X_train[:,:,21]**3 + 1e-10) + 1\n",
    "X_train[:,:,22] = 0.1 * np.log10(1e20*X_train[:,:,22]**4 + 1e-10) + 1\n",
    "X_train[:,:,23] = 0.1 * np.log10(1e20*X_train[:,:,23]**4 + 1e-10) + 1\n",
    "X_train[:,:,24] = 0.1 * np.log10(1e10*X_train[:,:,24]**5 + 1e-10) + 1\n",
    "X_train[:,:,26] = 0.1 * np.log10(1e10*X_train[:,:,26]**7 + 1e-10) + 1\n",
    "X_train[:,:,27] = 0.1 * np.log10(1e10*X_train[:,:,27]**3 + 1e-10) + 1\n",
    "X_train[:,:,29] = 0.1 * np.log10(1e10*X_train[:,:,29]**7 + 1e-10) + 1\n",
    "X_train[:,:,30] = 0.1 * np.log10(1e10*X_train[:,:,30]**3 + 1e-10) + 1\n",
    "X_train[:,:,32] = 0.1 * np.log10(1e10*X_train[:,:,32]**3 + 1e-10) + 1\n",
    "X_train[:,:,33] = 0.1 * np.log10(1e10*X_train[:,:,33]**2.5 + 1e-10) + 1\n",
    "X_train[:,:,34] = 0.1 * np.log10(1e10*X_train[:,:,34]**3 + 1e-10) + 1\n",
    "X_train[:,:,35] = 0.1 * np.log10(1e20*X_train[:,:,35]**4 + 1e-10) + 1\n",
    "X_train[:,:,36] = 0.1 * np.log10(1e18*X_train[:,:,36]**5 + 1e-10) + 1\n",
    "X_train[:,:,37] = 0.1 * np.log10(1e20*X_train[:,:,37]**4 + 1e-10) + 1\n",
    "X_train[:,:,38] = 0.1 * np.log10(1e20*X_train[:,:,38]**3 + 1e-10) + 1\n",
    "X_train[:,:,39] = 0.1 * np.log10(1e20*X_train[:,:,39]**3 + 1e-10) + 1\n",
    "X_train[:,:,40] = 0.1 * np.log10(1e20*X_train[:,:,40]**3 + 1e-10) + 1\n",
    "X_train[:,:,41] = 0.1 * np.log10(1e20*X_train[:,:,41]**3 + 1e-10) + 1\n",
    "X_train[:,:,42] = 0.1 * np.log10(1e20*X_train[:,:,42]**3 + 1e-10) + 1\n",
    "X_train[:,:,43] = 0.1 * np.log10(1e20*X_train[:,:,43]**3 + 1e-10) + 1\n",
    "X_train[:,:,44] = 0.1 * np.log10(1e20*X_train[:,:,44]**3 + 1e-10) + 1\n",
    "X_train[:,:,45] = 0.1 * np.log10(1e20*X_train[:,:,45]**3 + 1e-10) + 1\n",
    "X_train[:,:,46] = 0.1 * np.log10(1e20*X_train[:,:,46]**3 + 1e-10) + 1\n",
    "X_train[:,:,47] = 0.1 * np.log10(1e20*X_train[:,:,47]**3 + 1e-10) + 1\n",
    "X_train[:,:,51] = 0.1 * np.log10(1e20*X_train[:,:,51]**3 + 1e-10) + 1\n",
    "X_train[:,:,52] = 0.1 * np.log10(1e20*X_train[:,:,52]**3 + 1e-10) + 1\n",
    "X_train[:,:,53] = 0.1 * np.log10(1e20*X_train[:,:,53]**3 + 1e-10) + 1\n",
    "X_train[:,:,54] = 0.1 * np.log10(1e20*X_train[:,:,54]**3 + 1e-10) + 1\n",
    "X_train[:,:,57] = 0.1 * np.log10(1e20*X_train[:,:,57]**20 + 1e-10) + 1\n",
    "X_train[:,:,58] = 0.1 * np.log10(1e20*X_train[:,:,58]**10 + 1e-10) + 1\n",
    "X_train[:,:,59] = 0.1 * np.log10(1e20*X_train[:,:,59]**8 + 1e-10) + 1\n",
    "X_train[:,:,60] = 0.1 * np.log10(1e20*X_train[:,:,60]**6 + 1e-10) + 1\n",
    "X_train[:,:,61] = 0.1 * np.log10(1e20*X_train[:,:,61]**6 + 1e-10) + 1\n",
    "X_train[:,:,62] = 0.1 * np.log10(1e20*X_train[:,:,62]**5 + 1e-10) + 1\n",
    "X_train[:,:,63] = 0.1 * np.log10(1e20*X_train[:,:,63]**3 + 1e-10) + 1\n",
    "X_train[:,:,64] = 0.1 * np.log10(1e20*X_train[:,:,64]**3 + 1e-10) + 1\n",
    "X_train[:,:,65] = 0.1 * np.log10(1e20*X_train[:,:,65]**3 + 1e-10) + 1\n",
    "X_train[:,:,66] = 0.1 * np.log10(1e20*X_train[:,:,66]**3 + 1e-10) + 1\n",
    "X_train[:,:,67] = 0.1 * np.log10(1e20*X_train[:,:,67]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,68] = 0.1 * np.log10(1e20*X_train[:,:,68]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,69] = 0.1 * np.log10(1e20*X_train[:,:,69]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,70] = 0.1 * np.log10(1e20*X_train[:,:,70]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,71] = 0.1 * np.log10(1e20*X_train[:,:,71]**3 + 1e-10) + 1\n",
    "X_train[:,:,72] = 0.1 * np.log10(1e20*X_train[:,:,72]**3 + 1e-10) + 1\n",
    "X_train[:,:,73] = 0.1 * np.log10(1e20*X_train[:,:,73]**3 + 1e-10) + 1\n",
    "X_train[:,:,74] = 0.1 * np.log10(1e20*X_train[:,:,74]**3 + 1e-10) + 1\n",
    "X_train[:,:,75] = 0.1 * np.log10(1e20*X_train[:,:,75]**3 + 1e-10) + 1\n",
    "X_train[:,:,76] = 0.1 * np.log10(1e20*X_train[:,:,76]**3 + 1e-10) + 1\n",
    "X_train[:,:,77] = 0.1 * np.log10(1e15*X_train[:,:,77]**8 + 1e-10) + 1\n",
    "X_train[:,:,78] = 0.1 * np.log10(1e15*X_train[:,:,78]**8 + 1e-10) + 1\n",
    "X_train[:,:,79] = 0.1 * np.log10(1e20*X_train[:,:,79]**5 + 1e-10) + 1\n",
    "X_train[:,:,80] = 0.1 * np.log10(1e20*X_train[:,:,80]**6 + 1e-10) + 1\n",
    "X_train[:,:,81] = 0.1 * np.log10(1e20*X_train[:,:,81]**8 + 1e-10) + 1\n",
    "X_train[:,:,82] = 0.1 * np.log10(1e20*X_train[:,:,82]**10 + 1e-10) + 1\n",
    "X_train[:,:,83] = 0.1 * np.log10(1e20*X_train[:,:,83]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,84] = 0.1 * np.log10(1e20*X_train[:,:,84]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,85] = 0.1 * np.log10(1e20*X_train[:,:,85]**3 + 1e-10) + 1\n",
    "X_train[:,:,86] = 0.1 * np.log10(1e20*X_train[:,:,86]**13 + 1e-10) + 1\n",
    "X_train[:,:,87] = 0.1 * np.log10(1e20*X_train[:,:,87]**10 + 1e-10) + 1\n",
    "X_train[:,:,88] = 0.1 * np.log10(1e20*X_train[:,:,88]**9 + 1e-10) + 1\n",
    "X_train[:,:,89] = 0.1 * np.log10(1e20*X_train[:,:,89]**8 + 1e-10) + 1\n",
    "X_train[:,:,91] = X_train[:,:,91] ** 5\n",
    "X_train[:,:,93] = X_train[:,:,93] ** 2\n",
    "X_train[:,:,94] = X_train[:,:,94] ** 2\n",
    "X_train[:,:,96] = 0.1 * np.log10(1e20*X_train[:,:,96]**4 + 1e-10) + 1\n",
    "X_train[:,:,97] = 0.1 * np.log10(1e20*X_train[:,:,97]**9 + 1e-10) + 1\n",
    "X_train[:,:,98] = 0.1 * np.log10(1e20*X_train[:,:,98]**8 + 1e-10) + 1\n",
    "X_train[:,:,99] = 0.1 * np.log10(1e20*X_train[:,:,99]**7 + 1e-10) + 1\n",
    "X_train[:,:,100] = 0.1 * np.log10(1e20*X_train[:,:,100]**7 + 1e-10) + 1\n",
    "X_train[:,:,101] = 0.1 * np.log10(1e20*X_train[:,:,101]**5 + 1e-10) + 1\n",
    "X_train[:,:,102] = 0.1 * np.log10(1e20*X_train[:,:,102]**4 + 1e-10) + 1\n",
    "X_train[:,:,103] = 0.1 * np.log10(1e20*X_train[:,:,103]**4 + 1e-10) + 1\n",
    "X_train[:,:,104] = 0.1 * np.log10(1e20*X_train[:,:,104]**10 + 1e-10) + 1\n",
    "X_train[:,:,106] = 0.1 * np.log10(1e20*X_train[:,:,106]**4 + 1e-10) + 1\n",
    "X_train[:,:,107] = 0.1 * np.log10(1e20*X_train[:,:,107]**8 + 1e-10) + 1\n",
    "X_train[:,:,108] = 0.1 * np.log10(1e20*X_train[:,:,108]**7 + 1e-10) + 1\n",
    "X_train[:,:,109] = 0.1 * np.log10(1e20*X_train[:,:,109]**7 + 1e-10) + 1\n",
    "X_train[:,:,110] = 0.1 * np.log10(1e20*X_train[:,:,110]**7 + 1e-10) + 1\n",
    "X_train[:,:,111] = 0.1 * np.log10(1e20*X_train[:,:,111]**6 + 1e-10) + 1\n",
    "X_train[:,:,112] = 0.1 * np.log10(1e20*X_train[:,:,112]**5 + 1e-10) + 1\n",
    "X_train[:,:,113] = 0.1 * np.log10(1e20*X_train[:,:,113]**5 + 1e-10) + 1\n",
    "X_train[:,:,114] = 0.1 * np.log10(1e20*X_train[:,:,114]**8 + 1e-10) + 1\n",
    "X_train[:,:,115] = 0.1 * np.log10(1e20*X_train[:,:,115]**7 + 1e-10) + 1\n",
    "X_train[:,:,116] = 0.1 * np.log10(1e20*X_train[:,:,116]**6 + 1e-10) + 1\n",
    "X_train[:,:,117] = 0.1 * np.log10(1e20*X_train[:,:,117]**6 + 1e-10) + 1\n",
    "X_train[:,:,118] = 0.1 * np.log10(1e20*X_train[:,:,118]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,119] = 0.1 * np.log10(1e20*X_train[:,:,119]**4 + 1e-10) + 1\n",
    "X_train[:,:,124] = 0.1 * np.log10(1e20*X_train[:,:,124]**6 + 1e-10) + 1\n",
    "X_train[:,:,125] = 0.1 * np.log10(1e20*X_train[:,:,125]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,126] = 0.1 * np.log10(1e20*X_train[:,:,126]**11 + 1e-10) + 1\n",
    "X_train[:,:,127] = 0.1 * np.log10(1e20*X_train[:,:,127]**8 + 1e-10) + 1\n",
    "X_train[:,:,128] = 0.1 * np.log10(1e20*X_train[:,:,128]**8 + 1e-10) + 1\n",
    "X_train[:,:,129] = 0.1 * np.log10(1e20*X_train[:,:,129]**8 + 1e-10) + 1\n",
    "X_train[:,:,130] = 0.1 * np.log10(1e20*X_train[:,:,130]**8 + 1e-10) + 1\n",
    "X_train[:,:,132] = 0.1 * np.log10(1e12*X_train[:,:,132]**12 + 1e-10) + 1\n",
    "X_train[:,:,133] = 0.1 * np.log10(1e20*X_train[:,:,133]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,140] = 0.1 * np.log10(1e20*X_train[:,:,140]**4 + 1e-10) + 1\n",
    "X_train[:,:,141] = 0.1 * np.log10(1e20*X_train[:,:,141]**4 + 1e-10) + 1\n",
    "X_train[:,:,142] = 0.1 * np.log10(1e20*X_train[:,:,142]**4 + 1e-10) + 1\n",
    "X_train[:,:,143] = 0.1 * np.log10(1e20*X_train[:,:,143]**4 + 1e-10) + 1\n",
    "X_train[:,:,144] = 0.1 * np.log10(1e20*X_train[:,:,144]**10 + 1e-10) + 1\n",
    "X_train[:,:,146] = 0.1 * np.log10(1e20*X_train[:,:,146]**10 + 1e-10) + 1\n",
    "X_train[:,:,147] = 0.1 * np.log10(1e20*X_train[:,:,147]**9 + 1e-10) + 1\n",
    "X_train[:,:,148] = 0.1 * np.log10(1e20*X_train[:,:,148]**8.5 + 1e-10) + 1\n",
    "X_train[:,:,149] = 0.1 * np.log10(1e20*X_train[:,:,149]**9 + 1e-10) + 1\n",
    "X_train[:,:,150] = 0.1 * np.log10(1e20*X_train[:,:,150]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,151] = 0.1 * np.log10(1e20*X_train[:,:,151]**12 + 1e-10) + 1\n",
    "X_train[:,:,152] = 0.1 * np.log10(1e20*X_train[:,:,152]**20 + 1e-10) + 1\n",
    "X_train[:,:,153] = 0.1 * np.log10(1e20*X_train[:,:,153]**11 + 1e-10) + 1\n",
    "X_train[:,:,154] = 0.1 * np.log10(1e20*X_train[:,:,154]**20 + 1e-10) + 1\n",
    "X_train[:,:,155] = 0.1 * np.log10(1e20*X_train[:,:,155]**16 + 1e-10) + 1\n",
    "X_train[:,:,156] = 0.1 * np.log10(1e20*X_train[:,:,156]**10 + 1e-10) + 1\n",
    "X_train[:,:,157] = 0.1 * np.log10(1e20*X_train[:,:,157]**11 + 1e-10) + 1\n",
    "X_train[:,:,158] = 0.1 * np.log10(1e20*X_train[:,:,158]**9 + 1e-10) + 1\n",
    "X_train[:,:,159] = 0.1 * np.log10(1e20*X_train[:,:,159]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,160] = 0.1 * np.log10(1e20*X_train[:,:,160]**11 + 1e-10) + 1\n",
    "X_train[:,:,166] = 0.1 * np.log10(1e20*X_train[:,:,166]**11 + 1e-10) + 1\n",
    "X_train[:,:,168] = 0.1 * np.log10(1e20*X_train[:,:,168]**14 + 1e-10) + 1\n",
    "X_train[:,:,173] = 0.1 * np.log10(1e30*X_train[:,:,173]**7 + 1e-10) + 1\n",
    "X_train[:,:,174] = 0.1 * np.log10(1e30*X_train[:,:,174]**4 + 1e-10) + 1\n",
    "X_train[:,:,178] = 0.1 * np.log10(1e30*X_train[:,:,178]**4 + 1e-10) + 1\n",
    "X_train[:,:,179] = 0.1 * np.log10(1e30*X_train[:,:,179]**4 + 1e-10) + 1\n",
    "X_train[:,:,181] = 0.1 * np.log10(1e30*X_train[:,:,181]**7 + 1e-10) + 1\n",
    "X_train[:,:,182] = 0.1 * np.log10(1e30*X_train[:,:,182]**2 + 1e-10) + 1\n",
    "X_train[:,:,183] = 0.1 * np.log10(1e30*X_train[:,:,183]**2 + 1e-10) + 1\n",
    "X_train[:,:,185] = 0.1 * np.log10(1e30*X_train[:,:,185]**2 + 1e-10) + 1\n",
    "X_train[:,:,186] = 0.1 * np.log10(1e30*X_train[:,:,186]**2 + 1e-10) + 1\n",
    "X_train[:,:,187] = 0.1 * np.log10(1e30*X_train[:,:,187]**2 + 1e-10) + 1\n",
    "X_train[:,:,190] = 0.1 * np.log10(1e10*X_train[:,:,190]**4 + 1e-10) + 1\n",
    "X_train[:,:,191] = 0.1 * np.log10(1e20*X_train[:,:,191]**6 + 1e-10) + 1\n",
    "X_train[:,:,192] = 0.1 * np.log10(1e20*X_train[:,:,192]**4 + 1e-10) + 1\n",
    "X_train[:,:,195] = 0.1 * np.log10(1e20*X_train[:,:,195]**4 + 1e-10) + 1\n",
    "X_train[:,:,196] = 0.1 * np.log10(1e20*X_train[:,:,196]**6 + 1e-10) + 1\n",
    "X_train[:,:,197] = 0.1 * np.log10(1e20*X_train[:,:,197]**4 + 1e-10) + 1\n",
    "X_train[:,:,198] = 0.1 * np.log10(1e20*X_train[:,:,198]**6 + 1e-10) + 1\n",
    "X_train[:,:,200] = 0.1 * np.log10(1e20*X_train[:,:,200]**6 + 1e-10) + 1\n",
    "X_train[:,:,201] = 0.1 * np.log10(1e20*X_train[:,:,201]**5 + 1e-10) + 1\n",
    "X_train[:,:,202] = 0.1 * np.log10(1e20*X_train[:,:,202]**5 + 1e-10) + 1\n",
    "X_train[:,:,203] = 0.1 * np.log10(1e20*X_train[:,:,203]**5 + 1e-10) + 1\n",
    "X_train[:,:,204] = 0.1 * np.log10(1e30*X_train[:,:,204]**3 + 1e-10) + 1\n",
    "X_train[:,:,205] = 0.1 * np.log10(1e30*X_train[:,:,205]**3 + 1e-10) + 1\n",
    "X_train[:,:,206] = 0.1 * np.log10(1e20*X_train[:,:,206]**7 + 1e-10) + 1\n",
    "X_train[:,:,207] = 0.1 * np.log10(1e30*X_train[:,:,207]**2 + 1e-10) + 1\n",
    "X_train[:,:,208] = 0.1 * np.log10(1e30*X_train[:,:,208]**2 + 1e-10) + 1\n",
    "X_train[:,:,209] = 0.1 * np.log10(1e30*X_train[:,:,209]**2 + 1e-10) + 1\n",
    "X_train[:,:,213] = 0.1 * np.log10(1e30*X_train[:,:,213]**2 + 1e-10) + 1\n",
    "X_train[:,:,214] = 0.1 * np.log10(1e7*X_train[:,:,214]**2 + 1e-10) + 1\n",
    "X_train[:,:,215] = 0.1 * np.log10(1e15*X_train[:,:,215]**2 + 1e-10) + 1\n",
    "X_train[:,:,216] = 0.1 * np.log10(1e15*X_train[:,:,216]**2 + 1e-10) + 1\n",
    "X_train[:,:,217] = 0.1 * np.log10(1e15*X_train[:,:,217]**2 + 1e-10) + 1\n",
    "X_train[:,:,218] = 0.1 * np.log10(1e15*X_train[:,:,218]**2 + 1e-10) + 1\n",
    "X_train[:,:,220] = 0.1 * np.log10(1e15*X_train[:,:,220]**2 + 1e-10) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Numpy tensor into PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append((X_train[i], y_train[i]))\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append((X_test[i], y_test[i]))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = ConvNet(in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "\n",
    "# classifier = GRU(in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "# classifier.apply(weight_init)\n",
    "# focal_loss = FocalLoss2(alpha, gamma_pos, gamma_neg)\n",
    "# if optim == \"adam\":\n",
    "#     optim_clsfr = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), \n",
    "#                              lr=learn_rate)\n",
    "# else:\n",
    "#     optim_clsfr = RangerLars(filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "#                              lr=learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(pred_y_list, label_list, save_path=False, stat='Train'):\n",
    "    ###########################################################\n",
    "    plt.ylabel('Log Count')\n",
    "    plt.xlabel('Score')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['left'].set_color('none')\n",
    "    ax.spines['bottom'].set_color('none')\n",
    "    \n",
    "    MIN = 0.4\n",
    "    MAX = 0.5\n",
    "    BIN = 100\n",
    "    plt.grid(color = '#9999CC')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 0)], \n",
    "             bins=[n/(10*BIN) for n in range(int(10*MIN)*BIN, int(10*MAX)*BIN)], \n",
    "             label='Negative',\n",
    "             color='#598987')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 1)], \n",
    "             bins=[n/(10*BIN) for n in range(int(10*MIN)*BIN, int(10*MAX)*BIN)], \n",
    "             label='Positive', \n",
    "             color='#FFD000')\n",
    "    plt.legend(loc='upper right')\n",
    "    if save_path:\n",
    "        plt.savefig(\"result/{}_{}.jpg\".format(save_path, stat), dpi=1000, quality=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(PATH, save_path=False):\n",
    "    # optimizer = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), lr=learn_rate)\n",
    "\n",
    "    # model = GRU(in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "    model = ConvNet(fc_dims=[128, 64], in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    ##### Training #####\n",
    "    Training_label_list = []\n",
    "    Training_pred_y_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in tqdm_notebook(enumerate(train_dataloader)):\n",
    "        if data.size()[0] != train_dataloader.batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        # Update classifier\n",
    "\n",
    "        # pred_y = model(data).squeeze(-1)\n",
    "        # pred_y = torch.nn.functional.softmax(pred_y, dim=1)[:, 1]\n",
    "        \n",
    "        Training_pred_y = model(data.permute(0, 2, 1)).squeeze(-1)     # for Conv1D\n",
    "        Training_pred_y = torch.nn.functional.softmax(Training_pred_y, dim=1)[:, 1]\n",
    "\n",
    "        Training_label_list += list(target.cpu().detach().numpy())\n",
    "        Training_pred_y_list += list(Training_pred_y.cpu().detach().numpy())\n",
    "    \n",
    "    \n",
    "    Training_label_list = np.array(Training_label_list)\n",
    "    Training_pred_y_list = np.array(Training_pred_y_list)\n",
    "    auc_train = roc_auc_score(Training_label_list, Training_pred_y_list)\n",
    "    \n",
    "    ##### Testing #####\n",
    "    Testing_label_list = []\n",
    "    Testing_pred_y_list = []\n",
    "    for batch_idx, (data, target) in tqdm_notebook(enumerate(test_dataloader)):\n",
    "        if data.size()[0] != test_dataloader.batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        # Update classifier\n",
    "\n",
    "        # pred_y = model(data).squeeze(-1)\n",
    "        # pred_y = torch.nn.functional.softmax(pred_y, dim=1)[:, 1]\n",
    "        \n",
    "        Testing_pred_y = model(data.permute(0, 2, 1)).squeeze(-1)     # for Conv1D\n",
    "        Testing_pred_y = torch.nn.functional.softmax(Testing_pred_y, dim=1)[:, 1]\n",
    "\n",
    "        Testing_label_list += list(target.cpu().detach().numpy())\n",
    "        Testing_pred_y_list += list(Testing_pred_y.cpu().detach().numpy())\n",
    "    \n",
    "    Testing_label_list = np.array(Testing_label_list)\n",
    "    Testing_pred_y_list = np.array(Testing_pred_y_list)\n",
    "    auc_test = roc_auc_score(Testing_label_list, Testing_pred_y_list)\n",
    "    \n",
    "    ### Performance \n",
    "    ratio = .05\n",
    "    thres1 = np.min(Training_pred_y_list[np.where(Training_label_list == 1)])\n",
    "    thres2 = sorted(Testing_pred_y_list, reverse=True)[int(Testing_label_list.sum() + int(len(Testing_label_list)*ratio))]\n",
    "    \n",
    "    # thres2 = np.min(Testing_pred_y_list[np.where(y_test == 1)])\n",
    "    # thres3 = (8*thres1+2*thres2)/10 \n",
    "    # cut1, cut2 = thres1, thres2\n",
    "    \n",
    "    print ('Cut1: {}'.format(thres1))\n",
    "    print ('Cut2: {}'.format(thres2))\n",
    "\n",
    "    print(\"Recall of Training = 1 : {}\".format(thres1 > np.min(Training_pred_y_list)))\n",
    "    print('Training => auc: {:.6f}'.format(auc_train))\n",
    "    print (\"Training Treshold: {}\".format(thres1))\n",
    "    \n",
    "    y_predict_bin = np.array(Training_pred_y_list >= thres1, dtype=int)\n",
    "    prec_train, recall_train, f1_train = evaluate(Training_label_list, y_predict_bin)\n",
    "    print('prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.format(prec_train, recall_train, f1_train))\n",
    "    print (\"Total Positve: {}\".format(int(sum(Training_label_list))))\n",
    "    num_cand = np.sum(Training_pred_y_list >= thres1)\n",
    "    print (\"Total Candidate: {}\".format(num_cand))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    ## High Risk\n",
    "    print(\"High Risk Positve: {}\".format(np.sum(Training_label_list[Training_pred_y_list >= thres1])))\n",
    "    print(\"High Risk Candidate: {}\".format(np.sum(Training_pred_y_list >= thres1)))\n",
    "    print(\"High Risk Prec: {}\".format(np.sum(Training_label_list[np.where(Training_pred_y_list >= thres1)])/\\\n",
    "                                      np.sum(Training_pred_y_list >= thres1)))\n",
    "    print('----------------------------------------------')\n",
    "    ## Medium Risk\n",
    "    print(\"Medium Risk Positve: {}\".format(np.sum(Training_label_list[(Training_pred_y_list < thres1) &\\\n",
    "                                                                     (Training_pred_y_list >= thres2)])))\n",
    "    print(\"Medium Risk Candidate: {}\".format(np.sum((Training_pred_y_list < thres1) &\\\n",
    "                                                    (Training_pred_y_list >= thres2))))\n",
    "    print(\"Medium Risk Prec: {}\".format(np.sum(Training_label_list[np.where((Training_pred_y_list < thres1) &\\\n",
    "                                                                           (Training_pred_y_list >= thres2))])/\\\n",
    "                                                             np.sum((Training_pred_y_list < thres1) &\\\n",
    "                                                                    (Training_pred_y_list >= thres2))))\n",
    "    \n",
    "    plotting(Training_pred_y_list, Training_label_list, save_path)\n",
    "#     pd.DataFrame([Training_pred_y_list, Training_label_list, training_announce.tolist()], \n",
    "#                  index=['score', 'label', 'annouce']).T.to_csv('result/Training_result.csv', index=None)\n",
    "    ####　----------------------------------------------\n",
    "    print(\"Recall of Testing = 1 : {}\".format(thres2 <= np.min(Testing_pred_y_list[Testing_label_list == 1])))\n",
    "    print('Testing => auc: {:.6f}'.format(auc_test))\n",
    "    print(\"Threshold is set to {}\".format(thres2))\n",
    "    \n",
    "    y_predict_bin = np.array(Testing_pred_y_list >= thres2, dtype=int)\n",
    "    prec_test, recall_test, f1_test = evaluate(Testing_label_list, y_predict_bin)\n",
    "    print('prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.format(prec_test, recall_test, f1_test))\n",
    "    print (\"Total Positve: {}\".format(int(sum(Testing_label_list))))\n",
    "    print(\"Total Candidate: {}\".format(np.sum(Testing_pred_y_list >= thres2)))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    ## High Risk\n",
    "    print(\"High Risk Positve: {}\".format(np.sum(Testing_label_list[Testing_pred_y_list >= thres1])))\n",
    "    print(\"High Risk Candidate: {}\".format(np.sum(Testing_pred_y_list >= thres1)))\n",
    "    print(\"High Risk Prec: {}\".format(np.sum(Testing_label_list[np.where(Testing_pred_y_list >= thres1)])/\\\n",
    "                                      np.sum(Testing_pred_y_list >= thres1)))\n",
    "    print('----------------------------------------------')\n",
    "    ## Medium Risk\n",
    "    print(\"Medium Risk Positve: {}\".format(np.sum(Testing_label_list[(Testing_pred_y_list < thres1) &\\\n",
    "                                                                     (Testing_pred_y_list >= thres2)])))\n",
    "    print(\"Medium Risk Candidate: {}\".format(np.sum((Testing_pred_y_list < thres1) &\\\n",
    "                                                    (Testing_pred_y_list >= thres2))))\n",
    "    print(\"Medium Risk Prec: {}\".format(np.sum(Testing_label_list[np.where((Testing_pred_y_list < thres1) &\\\n",
    "                                                                           (Testing_pred_y_list >= thres2))])/\\\n",
    "                                                             np.sum((Testing_pred_y_list < thres1) &\\\n",
    "                                                                    (Testing_pred_y_list >= thres2))))\n",
    "\n",
    "    \n",
    "    plotting(Testing_pred_y_list, Testing_label_list, save_path, stat='Test')\n",
    "#     pd.DataFrame([Testing_pred_y_list, Testing_label_list, testing_announce.tolist()], \n",
    "#                  index=['score', 'label', 'annouce']).T.to_csv('result/Testing_result.csv', index=None)\n",
    "    return Training_label_list, Training_pred_y_list, Testing_label_list, Testing_pred_y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_dim = [646, 1292, 646]\n",
      "fc_dim = [646/0.9, 128/0.8, 64/0.8, 2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9eed0eef5649fb80e346a4b38d2c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed6a371f5014ef09a57f8ce0b6b1291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cut1: 0.48975569009780884\n",
      "Cut2: 0.48886483907699585\n",
      "Recall of Training = 1 : True\n",
      "Training => auc: 1.000000\n",
      "Training Treshold: 0.48975569009780884\n",
      "prec: 1.0000, rec: 1.0000, F1score: 1.0000\n",
      "Total Positve: 103\n",
      "Total Candidate: 103\n",
      "----------------------------------------------\n",
      "High Risk Positve: 103.0\n",
      "High Risk Candidate: 103\n",
      "High Risk Prec: 1.0\n",
      "----------------------------------------------\n",
      "Medium Risk Positve: 0.0\n",
      "Medium Risk Candidate: 0\n",
      "Medium Risk Prec: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAaM0lEQVR4nO3dfZQU9Z3v8fcXGBxGUJCYRSUKXjELAuoAIbpKwLsCxjs+jF4E9RiMSHJW1iQoitm98WHPZdVjdC9iRAyg7lGDUVTGRSCYRVHRDA/CYojBSNRZl1UnPgGCPHzvH11TNMP0TE13V9f09Od1zhy6quvh+6MbPlP1q/qVuTsiIiIAHZIuQERE2g6FgoiIhBQKIiISUiiIiEhIoSAiIqFiDwXP9mfx4s1Zr1vMP6XY7lJsc6m2uxTbnGW7Myr2UMjazp17ki4hEaXY7lJsM5Rmu0uxzZDfdpdsKIiIyMEUCiIiElIoiIhIqFPSBYiIZLJ7927q6urYuXNnpOWPO243mzZtirmqtidTu8vLy+nduzdlZWWRt6VQEJE2q66ujm7dutGnTx/MrMXlP/nkS3r06FKAytqWptrt7tTX11NXV0ffvn0jb0unj0Skzdq5cyc9e/aMFAhyIDOjZ8+ekY+yGigURKRNUyBkL5u/O4WCiIiE1KcgkV165+3h68dumN7ifJF8S/+u5UOU76uZMXXqVH7+858DcNddd7Ft2zZuueWWvNYyY8YMfvrTn4bTp59+Oq+++mpe9xGFjhRERJpxyCGHsHDhQj7++ONY9zNjxowDppMIBFAoSJYuvfP28Ke1y+f7tz2ROHXq1InJkydzzz33HPTeRx99xEUXXcSwYcMYNmwYr7zySjj/7LPPprKykh/84Accd9xxYahccMEFDBkyhJNOOok5c+YAMH36dL788ktOOeUULrvsMgC6du0KwCWXXMLixYvDfU6cOJGnnnqKvXv3Mm3aNIYNG8YZZ3yLBx54IC/tVSiIiLTgmmuu4dFHH+Wzzz47YP6PfvQjfvKTn1BbW8tTTz3FpEmTALj11ls566yzWLt2LRdeeCHvvfdeuM68efNYs2YNq1evZubMmdTX13P77bfTpUsX3njjDR599NED9jF+/HgWLFgAwFdffcULL7zAd7/7XebOncvhhx9ObW0tL7ywkgcffJAtW7bk3Fb1KYiItOCwww7jiiuuYObMmXTpsv9+gOXLl/P73/8+nP7888/54osvePnll3n66acBGDt2LD169AiXmTlzZvje+++/z+bNm+nZs2fGfZ9zzjlce+217Nq1iyVLljBixAi6dOnCsmXL2LBhA08++SR79+5j27Yv2Lx5c6vuSWhKmwoFM7sAOBf4OnCfuy9LuKSSV4hTPeqolmLw4x//mMrKSq688spw3r59+1i1atUBQQGpG8easmLFCpYvX86qVauoqKhg5MiRLd5HUF5ezsiRI1m6dCkLFixgwoQJ4T7uvfdexowZk9eb9mI/fWRm88zsQzPb2Gj+WDN7y8zeNrPpAO7+jLtfDUwELom7NhGRqI444gjGjRvH3Llzw3mjR49m1qxZ4fQbb7wBwBlnnMETTzwBwLJly/jkk08A+Oyzz+jRowcVFRX84Q9/4LXXXgvXLSsrY/fu3U3ue/z48cyfP5+VK1cyZswYAMaMGcP9998frvPHP/6R7du359zOQhwpPATMAh5pmGFmHYH7gLOBOqDWzBa5e8Nx2D8G74uIhFo6kox7mIvrrrvugBCYefv3uWbqHQweOIc9+8oYMWIEs2fP5uabb2bChAksWLCA73znOxx11FF069aNsWPHMnv2bAYPHsw3v/lNvv3tb4fbmjx5MoMHD6aysvKgfoXRo0dzxRVXcN5559G5c2cAJk2axJ///GcqKyvZs2cfvXp9nWeeeSbnNlqmw5x8MrM+wHPuPjCYPg24xd3HBNM3BYveHvz8xt2XZ9jWZGAywJQptw0ZNerirGr69NOddO9entW6xay17d6ydWvW++rbq1fG7WR6L31+vuizLl7HHbeb44/vF3n5vXudjh0LeAf0vh37X3eoCF/u2rWLjh070qlTJ373u9e5/vpreeml12Mro7l2v/POZt5998AB8aqr+2f8S0qqT+EY4P206TpgOPD3wN8Ch5vZCe4+u/GK7j4HmNMwmW0BCxduorq6f7arF60o7c5XP8J11ek3uD0b6b30+fmiz7p4bdq0qVW/+Rd8QLwdb+5/XTE0fLl5cx3jxo1j3759dO7cmXnz5sZaV3Ptrqgoa9X3IKlQaCql3N1nAjMLXYy0Hep0lvagX79+rFu3LukyspJUKNQB30ib7g18kFAtEhPdpCZSfJK6ea0W6Gdmfc2sMzAeWJRQLSIiEoj9SMHMHgdGAl8zszrgZnefa2ZTgKVAR2Ceu7/ZzGYab7MKqFq0aBFVVVVxlF0SdKpGRBqLPRTcfUKG+YuBxU29F2GbNUANcHUOpYmISCNt6o5mSU6hz/+rv0Gysr75y017ALzX7CIHOrnlCxg7duzIoEGD2LNnD/379+fhhx+moqKixfXSTZo0ialTpzJgwIA2M0R2JhoQT0SkGQ0D1W3cuJHOnTsze/ZBV8q36Je//CUDBgwA2s4Q2ZkoFEREIjrzzDN5++23Abj77rsZOPQSBg69hH+Z9RgA27dv59xzz+Xkk09m4MCB4eimI0eOZPXq1TkPkT148OC8DZGdSVGGgplVmdmcmpqapEsRkRKxZ88enn/+eQYNGsSaNWuYP38+r7/4EK+tmM+DDz3DunXrWLJkCUcffTTr169n48aNjB079oBt5DpEdm1tbd6GyM6kKEPB3WvcfbKuPBKRuDX8Zj906FCOPfZYrrrqKl5++WUuvPBCDj20C127VlB93ihWrlzJoEGDWL58OTfeeCMrV67k8MMPj7yfc845h9/+9rfs2rWL559//oAhsh955BFOOeUUhg8fTn19PZs3b46tvepoFhFpRsNv9ukyjRl34oknsmbNGhYvXsxNN93E6NGj+dnPfhZpP1GGyC6EojxSEBFJ0ogRI3jmmWfYsWMn27d/ydOLVnDmmWfywQcfUFFRweWXX87111/P2rVrD1q3rQyRnYmOFESkeLRwCWmhBsSrrKxk4sSJfGvE9wCYNPF8Tj31VJYuXcq0adPo0KEDZWVl3H///Qetm8sQ2e7OkUcemZchsjMpylDQHc0iUijbtm1rcv7UqVOZ+sMRB8wbM2ZMk6d5VqxYEb6+4447uOOOO5rcfllZGfX19Qes26FDB2bMmHHQpaxxKcpQ0B3NIiLxKMpQkNKjcZpECkOhUALS/0O9+ITzE6wk/xQW7Z+7Y1bAp6m1I9k8WVOhIG1WpvGRNG5S6SgvL6e+vp6ePXsqGFrJ3amvr6e8vHWPZFUolJgtW7ce9GhMkbaqd+/e1NXV8dFHH0VafseO3VRUlLW8YL589fH+1503FW6/jWRqd3l5Ob17927VtooyFHT1kUhpKCsro2/fvpGXL/hzqdcP2P+6f9aPjM9ZPttdlKGgq49EROKhO5pFRCRUlEcK0jJ1xopINnSkICIiIYWCiIiEFAoiIhJSn4K0G7q7WSR3RXmkoMdxiojEoyiPFHSfgohIPIoyFKRpugxVRHJVlKePREQkHgoFEREJKRRERCSkUBARkZBCQUREQgoFEREJFWUo6OY1EZF4FOV9Crp5TUQkHkV5pCAiIvFQKIiISEihICIiIYWCiIiEirKjWaQl6YMDXnzC+QlWIlJcdKQgIiIhhYKIiIR0+khKih7ZKdI8HSmIiEhIoSAiIqGiPH1kZlVA1aJFi6iqqkq6HGnjtmzdyqV3Ppt0GSJFoShDQWMfpeiZzCKSbzp9JCIiIYWCiIiEFAoiIhJSKIiISEihICIiIYWCiIiEFAoiIhJSKIiISEihICIiIYWCiIiEFAoiIhJSKIiISEihICIiIYWCiIiEFAoiIhJqMRTM7G+izCskM6syszk1NTVJliEi0u5EecjOvUBlhHkFo4fsiIjEI2MomNlpwOnAkWY2Ne2tw4COcRcmIiKF19yRQmega7BMt7T5nwMXx1mUiIgkI2MouPuLwItm9pC7v1vAmqQZei5zYaT/PT92w/QEKxEprCh9CoeY2RygT/ry7n5WXEWJiEgyooTCr4HZwC+BvfGWIyIiSYoSCnvc/f7YKxERkcRFuXmtxsz+zsyOMrMjGn5ir0xERAouypHC94I/p6XNc+D4/JcjkozmOvDV6SylpMVQcPe+hShERESS12IomNkVTc1390fyX46IiCQpyumjYWmvy4H/CawFFAoiIu1MlNNHf58+bWaHA/8aW0UiIpKYbIbO3gH0y3chIiKSvCh9CjWkrjaC1EB4/YEn4ixKRESSEaVP4a6013uAd929LqZ6REQkQVH6FF40s79if4fz5nhLEikMDS4ocrAoT14bB/wO+N/AOOB1M9PQ2SIi7VCU00f/AAxz9w8BzOxIYDnwZJyFiYhI4UW5+qhDQyAE6iOuJyIiRSbKkcISM1sKPB5MXwI8H19JIiKSlCgdzdPMrBo4AzBgjrs/HXtlIiJScBlDwcxOAP7K3V9x94XAwmD+CDP7H+7+p0IVKSIihdFc38C/AF80MX9H8J6IiLQzzYVCH3ff0Himu68m9bzmvDKz481srpnpqiYRkYQ0FwrlzbzXJcrGzWyemX1oZhsbzR9rZm+Z2dtmNh3A3d9x96uibFdEROLRXCjUmtnVjWea2VXAmojbfwgY22j9jsB9wDnAAGCCmQ2IuD0REYmRuXvTb6SGtnga+Ir9ITAU6Axc6O5bI+3ArA/wnLsPDKZPA25x9zHB9E0A7v7PwfST7p7xjmkzmwxMBpgy5bYho0Zld3P1p5/upHv35g6GCm/L1v1/pX179WpxmWxUdDyUHXu357SNYhNXm9M/oyifXaG1xe943Are5i9X73/dZWjh9ttIa9tdXd3fMr2X8eojd/9v4HQzGwUMDGb/m7v/NvKem3YM8H7adB0w3Mx6Av8XONXMbmoIiSbqmgPMaZjMtoiFCzdRXd0/29Vjcemdz4avr6tu+lnA6ctkY0jX4azZ9npO2yg2cbU5/TOK8tkVWlv8jset4G1en3aS4+Ss/zvKWT7bHeU+hX8H/j0ve0tpKqHc3euBH+ZxPyIi0kpJDFdRB3wjbbo38EECdYiISCNJhEIt0M/M+ppZZ2A8sCiBOkREpJEoYx9lzcweB0YCXzOzOuBmd59rZlOApaSe5DbP3d9s5XargKpFixZRVVWV77ILSmP6i0hbEuVxnF9wcIfuZ8Bq4Dp3fyfTuu4+IcP8xcDiVtTZeP0aoAY46JJZERHJXpQjhbtJnfN/jFQn8XigF/AWMI/UkYCIiLQDUfoUxrr7A+7+hbt/HlwS+l13XwD0iLk+EREpoCihsM/MxplZh+BnXNp7iVyYa2ZVZjanpqYmid2LiLRbUU4fXQb8P+AXwfQq4HIz6wJMiauw5qhPQUQkHlFuXnsHyHSJz8v5LUdERJLU4ukjM+ttZk8Ho53+t5k9ZWa9C1GciIgUVpQ+hfmkbi47mtS4RTXBPBERaWeihMKR7j7f3fcEPw8BR8Zcl4iIJCBKR/PHZnY58HgwPQGoj6+klrWnO5ozSb/T+bEb2saomxKNPjspZlFC4fvALOAeUpegvgpcGWdRLdHVRyIi8Wjx9JG7v+fu57n7ke7+dXe/AKguQG0iIlJg2Y6SOjWvVYiISJuQbShkfJSbiIgUr2xDIbnnzomISGwydjRnGDIbUkcJXWKrKIJSuPoonZ65ICKFkjEU3L1bIQtpDV19JCISjyQexykiIm2UQkFEREIKBRERCSkUREQkpFAQEZGQQkFEREJRBsRrc0rtPgVpfzKNpJrpnhSNtiqFUpShoPsURETiodNHIiISUiiIiEhIoSAiIiGFgoiIhBQKIiISUiiIiEhIoSAiIqGivE9BN69JW6CHH0l7VJShoJvXRETiodNHIiISUiiIiEhIoSAiIiGFgoiIhBQKIiISUiiIiEhIoSAiIiGFgoiIhBQKIiISUiiIiEioKIe50NhHUowyjZWkMZSkLSnKUNDYRyIi8dDpIxERCSkUREQkpFAQEZGQQkFEREIKBRERCSkUREQkpFAQEZGQQkFEREIKBRERCSkUREQkpFAQEZGQQkFEREIKBRERCSkUREQkVJRDZxfj8xTSx8x/7IbpCVYihRTHsxKa22b6d2vL1q1ceuezB80XaU5RhoKepyAiEg+dPhIRkZBCQUREQgoFEREJKRRERCSkUBARkZBCQUREQgoFEREJKRRERCSkUBARkZBCQUREQgoFEREJKRRERCSkUBARkZBCQUREQgoFEREJKRRERCSkUBARkZBCQUREQgoFEREJKRRERCSkUBARkZBCQUREQgoFEREJKRRERCTUKekCGpjZocAvgK+AFe7+aMIliYiUnFiPFMxsnpl9aGYbG80fa2ZvmdnbZjY9mF0NPOnuVwPnxVmXiIg0Le7TRw8BY9NnmFlH4D7gHGAAMMHMBgC9gfeDxfbGXJeIiDTB3D3eHZj1AZ5z94HB9GnALe4+Jpi+KVi0DvjE3Z8zs1+5+/gM25sMTAaYMuW2IaNGXZxVXZ9+upPu3cubXWbL1q2RttW3V6+c1i+kio6HsmPv9qTLKKhSbDMc2O7072j69zLTdzeTXNYthCj/rvPqy9X7X3cZWrj9NtLadldX97dM7yXRp3AM+48IIBUGw4GZwCwzOxeoybSyu88B5jRMZlvEwoWbqK7u3+wyl975bKRtXVc9vcn5UdcvpCFdh7Nm2+tJl1FQpdhmOLDd6d/R9O9lpu9uJrmsWwhR/l3n1foB+1+fHO8v2M3JZ7uTCIWmEsrdfTtwZaGLERGR/ZK4JLUO+EbadG/ggwTqEBGRRpIIhVqgn5n1NbPOwHhgUWs2YGZVZjanpibjWSYREclCrKePzOxxYCTwNTOrA25297lmNgVYCnQE5rn7m63ZrrvXkOp3uDrPJYuIlLRYQ8HdJ2SYvxhYHOe+RUSk9TTMhYiIhBQKIiISKspQUEeziEg8Yr+jua0ys8nBjXAlpRTbXYpthtJsdym2GfLb7qI8UsiTyUkXkJBSbHcpthlKs92l2GbIY7tLORRERKQRhYKIiIRKORRK7rxjoBTbXYpthtJsdym2GfLY7pLtaBYRkYOV8pGCiIg0olAQEZFQuwyFDM+Abmq5i83MzWxo2rybgvXeMrMxhak4d9m22czONrM1ZvYfwZ9nFa7q3OXyWQfzjzWzbWZ2ffzV5keO3+/BZrbKzN4MPvMCPqYsNzl8x8vM7OGgvZvSnvbY5rXUZjObaGYfmdkbwc+ktPe+Z2abg5/vRd6pu7erH1Ijr/4JOB7oDKwHBjSxXDfgJeA1YGgwb0Cw/CFA32A7HZNuU8xtPhU4Ong9EPjPpNtTiHanvfcU8Gvg+qTbU4DPuhOwATg5mO5ZDN/vPLT7UuBXwesK4M9An6TblI82AxOBWU2sewTwTvBnj+B1jyj7bY9HCt8C3nb3d9z9K+BXwPlNLPdPwJ3AzrR555P68uxy9y3A28H22rqs2+zu69y94SFHbwLlZnZI3AXnSS6fNWZ2Aal/LK0auj1hubR5NLDB3dcDuHu9u++Nu+A8yaXdDhxqZp2ALsBXwOcx15sPUdvclDHAb9z9L+7+CfAbYGyUFdtjKDT1DOhj0hcws1OBb7j7c61dt43Kpc3pLgLWufuu/JcYi6zbbWaHAjcCt8ZdZJ7l8lmfCLiZLTWztWZ2Q7yl5lUu7X4S2A78F/AecJe7/yXGWvMl6v9HF5nZBjN70swanmqZ9f9lSTyjOW5NPgM6fNOsA3APqcOuVq3bhuXS5oZlTgLuIPXbZLHIpd23Ave4+zazpjbTZuXS5k7AGcAwYAfwgpmtcfcXYqgz33Jp97eAvcDRpE6lrDSz5e7+Tgx15lOU/49qgMfdfZeZ/RB4GDgr4rpNao+h0NIzoLuROne+IvjPoBewyMzOi7BuW5V1m919tZn1Bp4GrnD3PxWo5nzI5bMeDlxsZncC3YF9ZrbT3WcVpPLs5fr9ftHdPwYws8VAJVAMoZBLuy8Flrj7buBDM3sFGErq1GFb1uL/R+5enzb5IKlf7BrWHdlo3RWR9pp0Z0oMnTOdSH3YfdnfOXNSM8uvYH+H1Ekc2NH8DkXQEZdjm7sHy1+UdDsK2e5G82+heDqac/msewBrSXW2dgKWA+cm3aYCtPtGYD6p354PBX4PDE66TfloM3BU2usLgdeC10cAW4LPvEfw+ogo+213fQruvgdoeAb0JuAJd3/TzG4Lfmtobt03gSdIfWmWANd4EXTE5dLmYL0TgP+Tdlnb12MuOS9ybHdRyvH7/QlwN1ALvAGsdfd/i7vmfMjxs74P6ApsJNX2+e6+IdaC8yBim68NLi9eD1xLcPrMU30m/0SqvbXAbR6xH0XDXIiISKjdHSmIiEj2FAoiIhJSKIiISEihICIiIYWCiIiEFAoiEZjZPwSX/m0ILtsdnnRNInFoj3c0i+SVmZ0G/C+g0lPDCXyN1M1E2W6vU3ANukiboyMFkZYdBXzswUCB7v6xu39gZsPM7FUzW29mvzOzbmZWbmbzg7H715nZKAjHvf+1mdUAy4J508ysNjj6KLaB+aSd0pGCSMuWAT8zsz+SGhpiAbAq+PMSd681s8OAL4EfAbj7IDP7a2CZmZ0YbOc0UsMr/MXMRgP9SA3WZqTG6Rnh7i8VtGUijehIQaQF7r4NGAJMBj4iFQY/AP7L3WuDZT4PTgmdAfxrMO8PwLukhqyGYHz74PXo4GcdqfGI/ppUSIgkSkcKIhEEY2CtIDUK538A19D0UMTNjcO9vdFy/+zuD+StSJE80JGCSAvM7Jtmlv5b/CmkBig72syGBct0C57s9RJwWTDvROBY4K0mNrsU+L6ZdQ2WPaZYBiKU9k1HCiIt6wrca2bdgT2kHtM6mdRwzPeaWRdS/Ql/C/wCmB0cTewBJgZXLB2wQXdfZmb9gVXBe9uAy4EPC9MkkaZplFQREQnp9JGIiIQUCiIiElIoiIhISKEgIiIhhYKIiIQUCiIiElIoiIhI6P8DA05lUbE/F2EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall of Testing = 1 : False\n",
      "Testing => auc: 0.983423\n",
      "Threshold is set to 0.48886483907699585\n",
      "prec: 0.1298, rec: 0.9444, F1score: 0.2282\n",
      "Total Positve: 18\n",
      "Total Candidate: 131\n",
      "----------------------------------------------\n",
      "High Risk Positve: 14.0\n",
      "High Risk Candidate: 61\n",
      "High Risk Prec: 0.22950819672131148\n",
      "----------------------------------------------\n",
      "Medium Risk Positve: 3.0\n",
      "Medium Risk Candidate: 70\n",
      "Medium Risk Prec: 0.04285714285714286\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH = '../../user_data/CloudMile/data/saved_models/FeatEmbed-Conv1Dsmall_v2_eps-21_focal62_rlars_lr4_epoch1600_Part2_0.19354838709677416'\n",
    "Training_label_list, Training_pred_y_list, Testing_label_list, Testing_pred_y_list= func(PATH, 'FeatEmbed-Golden-LastSoftmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
