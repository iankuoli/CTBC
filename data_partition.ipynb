{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_by_date(date_ym, offset=0, edge_type='aggregate', weight_type='embed_heter_superv_recur_focal_logisticMF'):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        date_ym: tuple (year, month)\n",
    "        offset: month_offset, one in [3, 6]\n",
    "        graph_type: one in ['latest', 'aggregate']\n",
    "        weight_type: one in ['weight_log', '']\n",
    "    \"\"\"\n",
    "    year, month = date_ym\n",
    "    \n",
    "    # Fetch graph embedding by customer ID\n",
    "    strrrr = 'graph_embed/offset_{}/{}/{}_{}-{}.npy'.format(offset, edge_type, weight_type, year, month)\n",
    "    print(strrrr)\n",
    "    E_t = np.load(strrrr)\n",
    "    print(E_t.shape)\n",
    "    embedding = pd.DataFrame(data=E_t, \n",
    "                             index=range(E_t.shape[0]),\n",
    "                             columns=['embed_{}_{}_{}'.format(edge_type, weight_type, i) for i in range(E_t.shape[1])])\n",
    "    with open(\"GraphLaplacian/offset_3/aggregate/weight_log_nodes_int2key_{}-{}.pickle\".\n",
    "              format(year, month), 'rb') as f:\n",
    "        int2key_t = pickle.load(f)\n",
    "    embedding['customerno'] = embedding.apply(lambda x: int2key_t[x.name], axis=1)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(date_ym, pre_fix=\"\", \n",
    "                use_feature_ctbc=True, use_feature_embed=True, use_feature_rankscore=True):\n",
    "    year = date_ym[0]\n",
    "    month = date_ym[1]\n",
    "    from_date = pd.to_datetime(\"{}/{}/{}\".format(month, 1, year))\n",
    "    to_date = from_date + pd.DateOffset(months=1)\n",
    "    print(\"Construct dataframe in {}.{}\".format(year, month))\n",
    "    \n",
    "    # Get CTBC features\n",
    "    if year == 2017:\n",
    "        file_name = 'aml_all_{}{:02d}_y.csv'.format(str(2018)[2:], 1)\n",
    "    else:\n",
    "        file_name = 'aml_all_{}{:02d}_y.csv'.format(str(year)[2:], month)\n",
    "    pdframe = pd.read_csv('../AMLAML/' + file_name)\n",
    "    pdframe = pdframe.set_index('customerno')\n",
    "    \n",
    "    # Attach graph embedding onto the features\n",
    "    # weight_type: [weight_none, weight_log, count_larger8000]\n",
    "    embed = get_embedding_by_date(date_ym, weight_type='embed_heter_superv_recur_focal_logisticMF_embed2')\n",
    "    pdframe = pdframe.join(embed.set_index('customerno'), on='customerno')\n",
    "    embed_col_names = list(embed.columns)\n",
    "    embed_col_names.remove('customerno')\n",
    "    pdframe.loc[:, embed_col_names] = pdframe.loc[:, embed_col_names].fillna(0)\n",
    "    \n",
    "    # Attach ranking scores onto the features\n",
    "    rscore = get_rankscore_by_date(date_ym, offset=3, edge_type='aggregate', weight_type='weight_log')\n",
    "    pdframe = pdframe.join(rscore.set_index('customerno'), on='customerno')\n",
    "    rscore = get_rankscore_by_date(date_ym, offset=6, edge_type='aggregate', weight_type='weight_log')\n",
    "    pdframe = pdframe.join(rscore.set_index('customerno'), on='customerno')\n",
    "    rscore = get_rankscore_by_date(date_ym, offset=3, edge_type='aggregate', weight_type='weight_none')\n",
    "    pdframe = pdframe.join(rscore.set_index('customerno'), on='customerno')\n",
    "    rscore = get_rankscore_by_date(date_ym, offset=6, edge_type='aggregate', weight_type='weight_none')\n",
    "    pdframe = pdframe.join(rscore.set_index('customerno'), on='customerno')\n",
    "    rscore = get_rankscore_by_date(date_ym, offset=3, edge_type='aggregate', weight_type='count_larger8000')\n",
    "    pdframe = pdframe.join(rscore.set_index('customerno'), on='customerno')\n",
    "    rscore = get_rankscore_by_date(date_ym, offset=6, edge_type='aggregate', weight_type='count_larger8000')\n",
    "    pdframe = pdframe.join(rscore.set_index('customerno'), on='customerno')\n",
    "    rscore = get_rankscore_by_date(date_ym, offset=6, edge_type='aggregate', weight_type='count_larger100000')\n",
    "    pdframe = pdframe.join(rscore.set_index('customerno'), on='customerno')\n",
    "    rscore_col_names = []\n",
    "    for x in pdframe.columns:\n",
    "        if x.startswith('pagerank') or x.startswith('divrank'):\n",
    "            rscore_col_names.append(x)\n",
    "    pdframe.loc[:, rscore_col_names] = pdframe.loc[:, rscore_col_names].fillna(0)\n",
    "    \n",
    "    # Attach label onto feature\n",
    "    target_list = SARCase[(SARCase.Status_SAR == 4) & \n",
    "                          (SARCase.created_date > from_date) & \n",
    "                          (SARCase.created_date < to_date)]['customerno'].unique()\n",
    "    # use ```x.name``` rather than ```x['customerno']``` since ```customerno``` has been the index not a column\n",
    "    pdframe['label'] = pdframe.apply(lambda x: 1 if x.name in target_list else 0, axis=1)\n",
    "    \n",
    "    return pdframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date_ym = DateYM(2018, 3)\n",
    "# to_date_ym = DateYM(2018, 1)\n",
    "to_date_ym = DateYM(2019, 6)\n",
    "list_date_seq = list_date_tuples(from_date_ym, to_date_ym)\n",
    "\n",
    "AML_data = {}\n",
    "embed_col_names = []\n",
    "pdframe_pre = None\n",
    "for date_ym in list_date_seq:\n",
    "    \n",
    "#     if pdframe_pre is None:\n",
    "#         date_pre = DateYM(date_ym[0], date_ym[1])\n",
    "#         date_pre.subtract_month(1)\n",
    "#         if date_pre.is_larger_than(DateYM(2018, 1)):\n",
    "#             pdframe_pre = get_feature(date_pre.export_tuple())\n",
    "#         else:\n",
    "#             pdframe_pre = get_feature((2018, 1))\n",
    "\n",
    "    pdframe = get_feature(date_ym)\n",
    "    \n",
    "#     cols = list(pdframe.columns)\n",
    "#     cols_for_diff = cols.copy()\n",
    "#     cols_for_diff.remove('Customer_Type_Code')\n",
    "#     cols_for_diff.remove('Customer_Category_Code')\n",
    "#     pdframe = pdframe.join(pdframe[cols_for_diff] - pdframe_pre[cols_for_diff] , \n",
    "#                            rsuffix='_diff', on='customerno')\n",
    "#     pdframe_pre = pdframe[cols]\n",
    "        \n",
    "    AML_data[date_ym] = pd.get_dummies(pdframe, columns=['Customer_Type_Code', 'Customer_Category_Code'])\n",
    "\n",
    "\n",
    "# Concatenate all the datasets\n",
    "# AML_dataset = pd.concat(list(AML_data.values()), axis=0)\n",
    "\n",
    "# One-hot Encode\n",
    "# AML_dataset = pd.get_dummies(AML_dataset, columns=['Customer_Type_Code', 'Customer_Category_Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct dataframe in 2018.3\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-3.npy\n",
    "(64712, 300)\n",
    "Construct dataframe in 2018.4\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-4.npy\n",
    "(65481, 300)\n",
    "Construct dataframe in 2018.5\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-5.npy\n",
    "(64407, 300)\n",
    "Construct dataframe in 2018.6\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-6.npy\n",
    "(66627, 300)\n",
    "Construct dataframe in 2018.7\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-7.npy\n",
    "(67397, 300)\n",
    "Construct dataframe in 2018.8\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-8.npy\n",
    "(66943, 300)\n",
    "Construct dataframe in 2018.9\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-9.npy\n",
    "(65069, 300)\n",
    "Construct dataframe in 2018.10\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-10.npy\n",
    "(65047, 300)\n",
    "Construct dataframe in 2018.11\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-11.npy\n",
    "(63888, 300)\n",
    "Construct dataframe in 2018.12\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2018-12.npy\n",
    "(63092, 300)\n",
    "Construct dataframe in 2019.1\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2019-1.npy\n",
    "(64078, 300)\n",
    "Construct dataframe in 2019.2\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2019-2.npy\n",
    "(59535, 300)\n",
    "Construct dataframe in 2019.3\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2019-3.npy\n",
    "(58637, 300)\n",
    "Construct dataframe in 2019.4\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2019-4.npy\n",
    "(57278, 300)\n",
    "Construct dataframe in 2019.5\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2019-5.npy\n",
    "(53995, 300)\n",
    "Construct dataframe in 2019.6\n",
    "graph_embed/offset_0/aggregate/embed_heter_superv_recur_focal_logisticMF_embed2_2019-6.npy\n",
    "(55511, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankscore_feat = []\n",
    "for x in list(pdframe.columns):\n",
    "    if (x.startswith('pagerank') or x.startswith('divrank')) and not x.endswith('_diff'):\n",
    "        rankscore_feat.append(x)\n",
    "print('The length of rankscore_feat = {}'.format(len(rankscore_feat)))\n",
    "\n",
    "embed_feat = []\n",
    "for x in list(pdframe.columns):\n",
    "    if x.startswith('embed') and not x.endswith('_diff'):\n",
    "        embed_feat.append(x)\n",
    "print('The length of embed_feat = {}'.format(len(embed_feat)))\n",
    "\n",
    "feature_feat = []\n",
    "for x in list(pdframe.columns):\n",
    "    if x.startswith('feature') or x.startswith('SARU'):\n",
    "        feature_feat.append(x)\n",
    "print('The length of feature_feat = {}'.format(len(feature_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScalarFrame = pd.DataFrame(index=rankscore_feat+embed_feat+feature_feat, columns=['max', 'min']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in AML_data.keys():\n",
    "    print (date)\n",
    "    tmp_AML_data = AML_data.get(date).describe()\n",
    "    for n in rankscore_feat:\n",
    "        if tmp_AML_data[n].max() > ScalarFrame.loc[n,'max']:\n",
    "            ScalarFrame.loc[n,'max'] = tmp_AML_data[n].max()\n",
    "        if tmp_AML_data[n].min() < ScalarFrame.loc[n,'min']:\n",
    "            ScalarFrame.loc[n,'min'] = tmp_AML_data[n].min()\n",
    "    for n in embed_feat:\n",
    "        if tmp_AML_data[n].max() > ScalarFrame.loc[n,'max']:\n",
    "            ScalarFrame.loc[n,'max'] = tmp_AML_data[n].max()\n",
    "        if tmp_AML_data[n].min() < ScalarFrame.loc[n,'min']:\n",
    "            ScalarFrame.loc[n,'min'] = tmp_AML_data[n].min()\n",
    "    for n in feature_feat:\n",
    "        if tmp_AML_data[n].max() > ScalarFrame.loc[n,'max']:\n",
    "            ScalarFrame.loc[n,'max'] = tmp_AML_data[n].max()\n",
    "        if tmp_AML_data[n].min() < ScalarFrame.loc[n,'min']:\n",
    "            ScalarFrame.loc[n,'min'] = tmp_AML_data[n].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2018, 3)\n",
    "(2018, 4)\n",
    "(2018, 5)\n",
    "(2018, 6)\n",
    "(2018, 7)\n",
    "(2018, 8)\n",
    "(2018, 9)\n",
    "(2018, 10)\n",
    "(2018, 11)\n",
    "(2018, 12)\n",
    "(2019, 1)\n",
    "(2019, 2)\n",
    "(2019, 3)\n",
    "(2019, 4)\n",
    "(2019, 5)\n",
    "(2019, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUframe_tmp\n",
    "# label     = np.array([])\n",
    "# announce  = np.array([])\n",
    "# FILTER    = np.array([])\n",
    "label    = {}\n",
    "announce = {}\n",
    "FILTER   = {}\n",
    "GRUdict  = {}\n",
    "for n in range(2, len(list_date_seq)):\n",
    "    # GRUframe_tmp = None\n",
    "    print (list_date_seq[n])\n",
    "    GRUframe_tmp_1 = AML_data[list_date_seq[n]]\n",
    "    # label = np.append(label, GRUframe_tmp_1['label'].values)\n",
    "    # announce = np.append(label, GRUframe_tmp_1['announce'].values)\n",
    "    # FILTER = np.append(label, GRUframe_tmp_1['FILTER'].values)\n",
    "    GRUframe_tmp_1 = GRUframe_tmp_1.drop('label', axis=1)\n",
    "    GRUframe_tmp_1[rankscore_feat] = (GRUframe_tmp_1[rankscore_feat] - ScalarFrame.T[rankscore_feat].min()) /(ScalarFrame.T[rankscore_feat].max() - ScalarFrame.T[rankscore_feat].min())\n",
    "    GRUframe_tmp_1[embed_feat] = (GRUframe_tmp_1[embed_feat] - ScalarFrame.T[embed_feat].min()) /(ScalarFrame.T[embed_feat].max() - ScalarFrame.T[embed_feat].min())\n",
    "    GRUframe_tmp_1[feature_feat] = (GRUframe_tmp_1[feature_feat] - ScalarFrame.T[feature_feat].min()) /(ScalarFrame.T[feature_feat].max() - ScalarFrame.T[feature_feat].min())\n",
    "    \n",
    "    \n",
    "    GRUframe_tmp_2 = AML_data[list_date_seq[n-1]]\n",
    "    GRUframe_tmp_2 = GRUframe_tmp_2.drop('label', axis=1)\n",
    "    GRUframe_tmp_2[rankscore_feat] = (GRUframe_tmp_2[rankscore_feat] - ScalarFrame.T[rankscore_feat].min()) /(ScalarFrame.T[rankscore_feat].max() - ScalarFrame.T[rankscore_feat].min())\n",
    "    GRUframe_tmp_2[embed_feat] = (GRUframe_tmp_2[embed_feat] - ScalarFrame.T[embed_feat].min()) /(ScalarFrame.T[embed_feat].max() - ScalarFrame.T[embed_feat].min())\n",
    "    GRUframe_tmp_2[feature_feat] = (GRUframe_tmp_2[feature_feat] - ScalarFrame.T[feature_feat].min()) /(ScalarFrame.T[feature_feat].max() - ScalarFrame.T[feature_feat].min())\n",
    "\n",
    "    \n",
    "    GRUframe_tmp_3 = AML_data[list_date_seq[n-2]]\n",
    "    GRUframe_tmp_3 = GRUframe_tmp_3.drop('label', axis=1)\n",
    "    GRUframe_tmp_3[rankscore_feat] = (GRUframe_tmp_3[rankscore_feat] - ScalarFrame.T[rankscore_feat].min()) /(ScalarFrame.T[rankscore_feat].max() - ScalarFrame.T[rankscore_feat].min())\n",
    "    GRUframe_tmp_3[embed_feat] = (GRUframe_tmp_3[embed_feat] - ScalarFrame.T[embed_feat].min()) /(ScalarFrame.T[embed_feat].max() - ScalarFrame.T[embed_feat].min())\n",
    "    GRUframe_tmp_3[feature_feat] = (GRUframe_tmp_3[feature_feat] - ScalarFrame.T[feature_feat].min()) /(ScalarFrame.T[feature_feat].max() - ScalarFrame.T[feature_feat].min())\n",
    "\n",
    "    \n",
    "    GRUdict[list_date_seq[n]] = np.hstack([GRUframe_tmp_1.values[:,np.newaxis, :], \n",
    "                                           GRUframe_tmp_2.values[:,np.newaxis, :], \n",
    "                                           GRUframe_tmp_3.values[:,np.newaxis, :]])\n",
    "    label[list_date_seq[n]]    = AML_data[list_date_seq[n]]['label'].values\n",
    "    announce[list_date_seq[n]] = AML_data[list_date_seq[n]]['announce'].values\n",
    "    FILTER[list_date_seq[n]]   = AML_data[list_date_seq[n]]['FILTER'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2018, 5)\n",
    "(2018, 6)\n",
    "(2018, 7)\n",
    "(2018, 8)\n",
    "(2018, 9)\n",
    "(2018, 10)\n",
    "(2018, 11)\n",
    "(2018, 12)\n",
    "(2019, 1)\n",
    "(2019, 2)\n",
    "(2019, 3)\n",
    "(2019, 4)\n",
    "(2019, 5)\n",
    "(2019, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_date   = [ (2018, 12),(2019, 1),(2019, 2),\n",
    "                  (2019, 3),(2019, 4), (2019, 5),(2019, 6)]\n",
    "training_date  = [(2018, 5),\n",
    "                  (2018, 6),(2018, 7), (2018, 8),\n",
    "                  (2018, 9),(2018, 10),(2018, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data     = np.concatenate([GRUdict[date] for date in training_date])\n",
    "testing_data      = np.concatenate([GRUdict[date] for date in testing_date])\n",
    "\n",
    "training_label    = np.concatenate([label[date] for date in training_date])\n",
    "testing_label     = np.concatenate([label[date] for date in testing_date])\n",
    "\n",
    "training_announce = np.concatenate([announce[date] for date in training_date])\n",
    "testing_announce  = np.concatenate([announce[date] for date in testing_date])\n",
    "\n",
    "training_FILTER   = np.concatenate([FILTER[date] for date in training_date])\n",
    "testing_FILTER    = np.concatenate([FILTER[date] for date in testing_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRUArray = np.vstack(list(GRUdict.values()))\n",
    "\n",
    "np.savez('datasets/Training_data_heter.npz', training_data, training_label, training_announce, training_FILTER)\n",
    "np.savez('datasets/Testing_data_heter.npz',  testing_data,  testing_label,  testing_announce,  testing_FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
